
\part{Outils d'analyse}

\begin{frame}{Plan}

\tableofcontents[subsectionstyle=hide]

\note{Ce qu'on va voir aujourd'hui va être essentiellement des rappels
  de ce que vous avez déjà vu. L'idée est d'aller un peu plus en
  profondeur et de voir aussi comment les idées qu'on a développées
  pour les algorithmes itératifs peut aussi s'appliquer pour les algorithmes récursifs.

\bigskip

On va aussi introduire certaines nouvelles notions.

\bigskip

Ce sera aussi l'occasion de voir deux techniques de preuves très importantes dans le cadre de ce cours: preuve par induction et preuve par contraposition
}

\end{frame}

\section{Correction d'algorithmes}

\begin{frame}{Plan}

\tableofcontents[currentsection]

\note{Ce qu'on va voir aujourd'hui va être essentiellement des rappels
  de ce que vous avez déjà vu. L'idée est d'aller un peu plus en
  profondeur et de voir aussi comment les idées qu'on a développées
  pour les algorithmes itératifs peut aussi s'appliquer pour les algorithmes récursifs.

\bigskip

On va aussi introduire certaines nouvelles notions.

\bigskip

Ce sera aussi l'occasion de voir deux techniques de preuves très importantes dans le cadre de ce cours: preuve par induction et preuve par contraposition
}

\end{frame}

\subsection{Introduction}

\begin{frame}{Analyse d'algorithmes}

Questions à se poser lors de la définition d'un algorithme:
\begin{itemize}
\item Mon algorithme est-il correct ?
\item Mon algorithme est-il efficace ? %en termes d'utilisation des
%  resources, temps CPU et/ou espace mémoire ?
\end{itemize}

\bigskip

Autres questions importantes seulement marginalement abordées dans ce cours:
\begin{itemize}
\item Modularité, fonctionnalité, robustesse, facilité d'utilisation, temps
  de programmation, simplicité, extensibilité, fiabilité,
  existence d'une solution algorithmique...
\end{itemize}

\end{frame}

\begin{frame}{Correction d'un algorithme}%, complétude, terminaison}

\begin{itemize}
\item La correction d'un algorithme s'étudie par rapport à un problème donné
\item Un problème est une collection d'instances de ce problème.
\begin{itemize}
\item Exemple de problème: trier un tableau
\item Exemple d'instance de ce problème: trier le tableau $[8,4,15,3]$
\end{itemize}
\item Un algorithme est correct pour une instance d'un problème s'il
  produit une solution correcte pour cette instance
\item Un algorithme est correct pour un problème s'il est correct pour
  toutes ses instances (on dira qu'il est totalement correct)
\item On s'intéressera ici à la correction d'un algorithme pour un
  problème (et pas pour seulement certaines de ses instances)
\end{itemize}

\note{}

\end{frame}

\begin{frame}{Comment vérifier la correction?}
\begin{itemize}
\item Première solution: en \alert{testant} concrètement l'algorithme:
\begin{itemize}
\item Suppose d'implémenter l'algorithme dans un langage (programme)
  et de le faire tourner
\item Suppose qu'on peut déterminer les instances du problème à vérifier
\item Il est très difficile de prouver empiriquement qu'on n'a pas de bug %On peut prouver qu'il y a un bug, pas qu'il n'y en a pas.
\end{itemize}
\item Deuxième solution: en dérivant une \alert{preuve mathématique} formelle:
\begin{itemize}
\item Pas besoin d'implémenter et de tester toutes les instances du problème
\item Sujet à des ``bugs'' également
\end{itemize}
\item En pratique, on combinera les deux

\bigskip

\item Outils pour prouver la correction d'un algorithme:
\begin{itemize}
%\item Pré-condition, post-condition
\item Algorithmes itératifs: triplets de Hoare, invariants de boucle% (induction)
\item Algorithmes récursifs: preuves par induction
\end{itemize}
\end{itemize}
\end{frame}

\subsection{Algorithmes itératifs}

\begin{frame}{Assertion}

\begin{itemize}
\item Relation entre les variables qui est vraie à un moment donné dans l'exécution
\item Assertions particulières:
\begin{itemize}
\item Pre-condition $P$: conditions que doivent remplir les entrées valides de l'algorithme
\item Post-condition $Q$: conditions qui expriment que le résultat de l'algorithme est correct
%\item Invariant: condition que doivent remplir
\end{itemize}
\item $P$ et $Q$ définissent resp. les instances et solutions valides du problème
\item Un code est correct si le triplet (de Hoare) $\{P\}$ code $\{Q\}$ est vrai.
\item Exemple:
$$\{x\geq 0\} y\gets \proc{sqrt}(x) \{y^2\isequal x\}$$
\end{itemize}

\end{frame}

\begin{frame}{Correction: séquence d'instructions}

\begin{center}\footnotesize
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi $\{P\}$
\zi S1
\zi S2
\zi \ldots
\zi Sn
\zi $\{Q\}$
\end{codebox}}
\end{center}

Pour vérifier que le triplet est correct:
\begin{itemize}
\item on insère des assertions $P_1$, $P_2$, \ldots, $P_n$ décrivant
  l'état des variables à chaque étape du programme
\item on vérifie que les triplets $\{P\}\mbox{ S1 }\{P_1\}$,
  $\{P_1\}\mbox{ S2 }\{P_2\}$, $\ldots$, $\{P_{n-1}\} \mbox{ Sn
}\{Q\}$ sont corrects
\end{itemize}

\medskip

Trois types d'instructions: affectation, condition, boucle

%\item Les instructions élémentaires sont généralement simples
%  à vérifier
%\item Les boucles sont traitées par la technique des invariants

\end{frame}

\begin{frame}{Correction: affectations}

Le triplet suivant est correct:
\begin{center}\footnotesize
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi $\{Q[x\rightarrow e]\}$
\zi $x\gets e$
\zi $\{Q\}$
\end{codebox}}
\end{center}
$Q[x\rightarrow e]$ est obtenu en remplaçant les occurrences de $x$ par $e$ dans $Q$.
\bigskip

Pour prouver un triplet:
\begin{center}\footnotesize
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi $\{P\}$
\zi $x\gets e$
\zi $\{Q\}$
\end{codebox}}
\end{center}
il faut montrer que $P$ implique $Q[x\rightarrow e]$.

\bigskip

Exemples: les triplets suivants sont corrects
\begin{columns}
\begin{column}{5cm}
\begin{center}\footnotesize
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi $\{x\isequal 2\}$
\zi $y\gets x+1$
\zi $\{y\isequal 3\}$
\end{codebox}}
\end{center}
\end{column}
\begin{column}{5cm}
\begin{center}\footnotesize
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi $\{x\isequal 42\}$
\zi $y\gets x+1$
\zi $z\gets y$ 
\zi $\{z\isequal 43\}$
\end{codebox}}
\end{center}
\end{column}
\end{columns}
%$\{x\isequal 2\} y\gets x+1\{y\isequal 3\}$

\note{On remplace $y$ par $x+1$ dans $y==3$, on obtient $x+1==3$, ce qui est correct étant donné la premier assertion}

\end{frame}

\begin{frame}{Correction: conditions}

\begin{center}\footnotesize
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi $\{P\}$
\zi \If $B$ \Then
\zi   C1
\zi \Else
\zi   C2\End
\zi $\{Q\}$
\end{codebox}}
\end{center}

Pour prouver que le triplet est correct, on doit prouver que
\begin{itemize}
\item $\{P\mbox{ et }B\}$ C1 $\{Q\}$
\item $\{P\mbox{ et non }B\}$ C2 $\{Q\}$
\end{itemize}
sont corrects

\bigskip

Exemple:
\begin{center}\footnotesize
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi $\{x<6\}$
\zi \If $x<0$ \Then
\zi   $y\gets 0$
\zi \Else
\zi   $y\gets x$\End
\zi $\{0\leq y<6 \}$
\end{codebox}}
\end{center}

\end{frame}

\begin{frame}{Correction: boucles}

\begin{columns}
\begin{column}{5cm}
\begin{center}\footnotesize
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi $\{P\}$
\zi INIT
\zi \While $B$ \Do
%\zi     \Do $\{I\}$
\zi      CORPS\End
\zi FIN
\zi $\{Q\}$
\end{codebox}}
\end{center}
\end{column}
\begin{column}{5cm}
\begin{center}\footnotesize
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi $\{P\}$
\zi INIT
\zi $\{I\}$
\zi \While $B$
\zi     \Do
      $\{I\mbox{ et }B\}$ CORPS $\{I\}$\End
\zi $\{I\mbox{ et non }B\}$
\zi FIN
\zi $\{Q\}$
\end{codebox}}
\end{center}
\end{column}
\end{columns}

Pour prouver que le triplet est correct:
\begin{itemize}
\item On met en évidence une assertion particulière $I$, appelée
  \alert{invariant de boucle}, qui décrit l'état du programme pendant
  la boucle.
\item On prouve que:
\begin{itemize}
\item $\{P\}$ INIT $\{I\}$ est correct%\hfill{\it(avant la boucle)}
\item $\{I \mbox{ et }B\}$ CORPS $\{I\}$ est correct%\hfill{\it(pendant la boucle)}
\item $\{I \mbox{ et non } B\}$ FIN $\{Q\}$ est correct%\hfill{\it(après la boucle)}
\end{itemize}
\end{itemize}

\bigskip

Si on a plusieurs boucles imbriquées, on les traite séparément, en
démarrant avec la boucle la plus interne.

\end{frame}

\begin{frame}{Correction: terminaison de boucle}

\begin{center}\footnotesize
\fcolorbox{white}{Lightgray}{
\begin{codebox}
%\zi $\{P\}$
\zi INIT
\zi \While $B$ \Do
%\zi     \Do $\{I\}$
\zi      CORPS\End
\zi FIN
%\zi $\{Q\}$
\end{codebox}}
\end{center}

\begin{itemize}
\item Un fois qu'on a prouvé que le triplet était correct, il faut encore montrer que la boucle se termine
\item Pour prouver la terminaison, on cherche une fonction de
  terminaison $f$:
\begin{itemize}
\item définie sur base des variables de l'algorithme et à valeur entière naturelle ($\geq 0$)
\item telle que $f$ décroît strictement suite à l'exécution du corps de la boucle
\item telle que $B$ implique $f>0$
\end{itemize}
\item Puisque $f$ décroît strictement, elle finira par atteindre 0 et donc à infirmer $B$.
\end{itemize}
\end{frame}

\begin{frame}{Exemple: $\proc{Fibonacci-iter}$}

\begin{columns}
\begin{column}{5cm}
\begin{center}\small
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\Procname{$\proc{Fibonacci-Iter}(n)$}
\zi \If $n \leq 1$
\zi \Then \Return n \End
\zi \Else
\zi \Then $pprev\gets 0$
\zi $prev\gets 1$
\zi \For $i\gets 2 \To n$
\zi \Do $f\gets prev+pprev$
\zi $pprev\gets prev$
\zi $prev\gets f$\End
\zi \Return f \End
\end{codebox}
}
\end{center}
Proposition: Si $n\geq 0$, $\proc{Fibonacci-iter}(n)$ renvoie $F_n$.
\end{column}
\begin{column}{5cm}
Réécriture, post- et pré-conditions
\begin{center}\small

\fcolorbox{white}{Lightgray}{
\begin{codebox}
\Procname{$\proc{Fibonacci-Iter}(n)$}
\zi {\color{red}$\{n\geq 0\}$} \Comment $\{P\}$
\zi \If $n \leq 1$
\zi \Then $prev\gets n$ \End
\zi \Else
\zi \Then $pprev\gets 0$
\zi $prev\gets 1$
\zi $i\gets 2$
\zi \While ($i\leq n$)
\zi \Do $f\gets prev+pprev$
\zi $pprev\gets prev$
\zi $prev\gets f$
\zi $i\gets i+1$\End\End
\zi {\color{red}$\{prev\isequal F_n\}$} \Comment $\{Q\}$
\zi \Return prev \End
\end{codebox}
}
\end{center}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Exemple: $\proc{Fibonacci-iter}$}

\begin{columns}
\begin{column}{5cm}
Analyse de la condition
\begin{center}
\small
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi {\color{red}$\{n\geq 0\mbox{ et } n\leq 1\}$}
\zi $prev\gets n$
\zi {\color{red}$\{prev\isequal F_n\}$}
\end{codebox}
}\\
{\color{darkgreen}correct} ($F_0=0, F_1=1$)

\bigskip

\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi {\color{red}$\{n\geq 0\mbox{ et } n>1\}$}
\zi $pprev\gets 0$
\zi $prev\gets 1$
\zi $i\gets 2$
\zi \While ($i\leq n$)
\zi \Do $f\gets prev+pprev$
\zi $pprev\gets prev$
\zi $prev\gets f$
\zi $i\gets i+1$\End\End
\zi {\color{red}$\{prev\isequal F_n\}$}
\end{codebox}}
\fcolorbox{black}{white}{\footnotesize
$I=\{pprev\isequal F_{i-2}, prev\isequal F_{i-1}\}$
}
\end{center}
\end{column}

\begin{column}{6cm}
Analyse de la boucle
\begin{center}\small
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi {\color{red}$\{n>1\}$}
\zi $pprev\gets 0$
\zi $prev\gets 1$
\zi $i\gets 2$
\zi {\color{red}$\{pprev\isequal F_{i-2}, prev\isequal F_{i-1}\}$}
\end{codebox}}\\
{\color{darkgreen}correct}% ($i=2\Rightarrow pprev=0=F_0, prev=1=F_1$)

\bigskip

\fcolorbox{white}{Lightgray}{\footnotesize
\begin{codebox}
\zi {\color{red}$\{pprev\isequal F_{i-2}, prev\isequal F_{i-1}, i\leq n\}$}
\zi $f\gets prev+pprev$
\zi $pprev\gets prev$
\zi $prev\gets f$
\zi $i\gets i+1$
\zi {\color{red}$\{pprev\isequal F_{i-2}, prev\isequal F_{i-1}\}$}
\end{codebox}}
{\color{darkgreen}correct}

\bigskip

\fcolorbox{white}{Lightgray}{\footnotesize
\begin{codebox}
\zi {\color{red}$\{pprev\isequal F_{i-2}, prev\isequal F_{i-1}, i\isequal n+1\}$}
\zi {\color{red}$\{prev\isequal F_n\}$}
\end{codebox}}\\
{\color{darkgreen}correct}

\end{center}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Exemple: $\proc{Fibonacci-iter}$}

\begin{center}
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\zi $i\gets 2$
\zi \While ($i\leq n$)
\zi \Do $f\gets prev+pprev$
\zi $pprev\gets prev$
\zi $prev\gets f$
\zi $i\gets i+1$\End
\end{codebox}}
\end{center}

\begin{itemize}
\item Fonction de terminaison $f=n-i+1$:
\begin{itemize}
\item $i\leq n \Rightarrow f=n-i+1>0$
\item $i=i+1 \Rightarrow$ $f$ diminue à chaque itération
\end{itemize}
\item L'algorithme est donc correct et se termine.
\end{itemize}
\qed
\end{frame}


%% \begin{frame}{Invariant}
%% \begin{itemize}
%% \item \alert{Invariant:} Une assertion qui définit ce qui est vrai
%%   avant et après chaque itération de la boucle
%% \item \alert{Initialisation:} Prouver que l'invariant est vrai avant la
%%   première itération (sous l'hypothèse que la pré-condition $P$ est
%%   vérifiée)
%% \item \alert{Maintenance:} Prouver que si l'invariant est vrai avant la
%%   $i$-ième itération, il l'est également après celle-ci, et par
%%   conséquent aussi avant la $i+1$-ième itération
%% \item \alert{Terminaison:} Prouver que si l'invariant est vrai après la
%%   dernière itération, la post-condition $Q$ est vérifiée
%% \end{itemize}

%% \centerline{\includegraphics[width=8cm]{Figures/02-invariant.pdf}}

%% \end{frame}

%% \begin{frame}{Mise en \oe uvre}
%% \begin{itemize}
%% \item Trouver un invariant de boucle une fois l'algorithme mis au point peut être assez complexe
%% \item Idéalement, l'invariant devrait être la propriété clé qui définit l'algorithme
%% \item Dans la suite du cours, on ne fournira un invariant que dans quelques cas
%% \end{itemize}
%% \end{frame}

\begin{frame}{Exemple: tri par insertion}

\begin{center}
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\Procname{$\proc{Insertion-Sort}(A)$}
\li \For $j \gets 2$ \To $\attrib{A}{length}$
\li     \Do
$\id{key} \gets A[j]$
\li \Comment Insert $A[j]$ into the sorted sequence
    $A[1 \twodots j-1]$.
\li $i \gets j-1$
\li \While $i > 0$ and $A[i] > \id{key}$
\li   \Do
        $A[i+1] \gets A[i]$
\li        $i \gets i-1$
    \End
\li $A[i+1] \gets \id{key}$
\End
\end{codebox}
}
\end{center}

\begin{itemize}
\item Démontrons informellement que la boucle externe est correcte
\item Invariant $I$: le sous-tableau $A[1\twodots j-1]$ contient les
  éléments du tableau original $A[1\twodots j-1]$ ordonnés.
\end{itemize}

\end{frame}

\begin{frame}{Exemple: tri par insertion}
\begin{columns}
\begin{column}{3cm}
\fcolorbox{white}{Lightgray}{\begin{codebox}
\zi \For $j \gets 2$ \To $A.length$
\zi     \Do $\ldots$
\End
\end{codebox}}
\end{column}
$\Leftrightarrow$
\begin{column}{5cm}
\fcolorbox{white}{Lightgray}{\begin{codebox}
\zi $j\gets 2$
\zi \While $i \leq A.length$
\zi     \Do $\ldots$
\zi     $j\gets j+1$
\End
\end{codebox}}
\end{column}
\end{columns}

\begin{itemize}
\item $P=$''$A$ est un tableau de taille $\attrib{A}{length}$'',\\
$Q=$''Le tableau $A$ est trié'',\\
$I=$''$A[1\twodots j-1]$ contient les $j-1$ premiers éléments de $A$ triés''

\bigskip

\item $\{P\} j=2 \{I\}$\hfill{\it(avant la boucle)}
\begin{itemize}
\item $j=2 \Rightarrow A[1]$ est trivialement ordonné
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Exemple: tri par insertion}
\begin{itemize}
\item $\{I\mbox{ et }j\leq \attrib{A}{length}\}$ CORPS $\{I\}$\hfill{\it(pendant la boucle)}
\begin{itemize}
\item La boucle interne déplace $A[j-1]$, $A[j-2]$, $A[j-3]\ldots$ d'une position vers la droite jusqu'à trouver la bonne position pour $key$ ($A[j]$).
\item $A[1\twodots j]$ contient alors les éléments originaux de $A[1\twodots j]$ triés.
\item $j=j+1$ rétablit l'invariant
\end{itemize}

\item $\{I\mbox{ et }j=A.length+1\}\{Q\}$\hfill{\it(après la boucle)}
\begin{itemize}
\item Puisque $j=A.length+1$, l'invariant implique que $A[1\twodots A.length]$ est ordonné.
\end{itemize}

\bigskip

\item Fonction de terminaison $f=\attrib{A}{length}-j+1$
\end{itemize}
\note{
\centerline{\includegraphics[width=8cm]{Figures/02-invinsertion.jpg}}
}
\end{frame}

\begin{frame}{Invariant}
\begin{itemize}
\item Un invariant peut être difficile à trouver pour certains algorithmes
\item En général, l'algorithme découle de l'invariant et pas l'inverse
\begin{itemize}
\item $\proc{Fibonacci-iter}$: On calcule itérativement $F_{i-1}$et $F_{i-2}$\\ ($I=\{pprev\isequal F_{i-2}, prev\isequal F_{i-1}\}$)
\item $\proc{Insertion-sort}$: On ajoute l'élément $j$ aux $j-1$ premiers éléments déjà triés\\ ($I=$''$A[1\twodots j-1]$ contient les $j-1$ premiers éléments de $A$ triés'')
\end{itemize}

\bigskip

\item La preuve par invariant est basée sur le principe général de preuve par induction qu'on va utiliser aussi pour prouver la correction des algorithmes récursifs
\end{itemize}

\centerline{\includegraphics[width=9cm]{Figures/02-invariant.pdf}}

\end{frame}

\subsection{Algorithmes récursifs}

%% \begin{frame}{Correction d'algorithmes récursifs}
%% \begin{itemize}
%% \item La correction d'algorithmes récursifs se fait naturellement par induction
%% \item La terminaison se prouve en montrant que le problème est réduit à chaque appel récursif
%% \end{itemize}
%% \end{frame}

\begin{frame}{Preuve par induction}

\centerline{\includegraphics[width=8cm]{Figures/02-induction.pdf}}

\begin{itemize}
\item On veut montrer qu'une propriété est vraie pour une série d'instances
\item On suppose l'existence d'un ordonnancement des instances
\item \alert{Cas de base:} on montre explicitement que la propriété est vraie pour la ou les premières instances
\item \alert{Cas inductif:} on suppose que la propriété est vraie pour les $k$ premières instances et on montre qu'elle l'est alors aussi pour la $k+1$-ième instance (quel que soit $k$)
\item Par le principe d'induction, la propriété sera vraie pour toutes les instances
\end{itemize}

\note{Exemple: instance = entier -> mettre l'exemple du slide suivant directement}

\end{frame}

\begin{frame}{Preuve par induction: exemple}

Proposition: Pour tout $n\geq 0$, on a $$\sum_{i=1}^n i=\frac{n(n+1)}{2}$$

Démonstration:
\begin{itemize}
\item Cas de base: $n=0\Rightarrow \sum_{i=1}^0 i=0=\frac{0(0+1)}{2}$
\item Cas inductif: Supposons la propriété vraie pour $n$ et montrons qu'elle est vraie pour $n+1$:
\begin{eqnarray*}
\sum_{i=1}^{n+1} i &= &\left(\sum_{i=1}^n i\right) + (n+1)=\frac{n(n+1)}{2}+(n+1)\\
&=& \frac{(n+1)(n+2)}{2}
\end{eqnarray*}
\item Par induction, le propriété est vraie pour tout $n$.
\end{itemize}\qed

\end{frame}

\begin{frame}{Correction d'algorithmes récursifs par induction}
\begin{itemize}
\item Propriété à montrer: l'algorithme est correct pour une instance quelconque du problème
\item Instances du problème ordonnées par ``taille'' (taille du tableau, nombre de bits, un entier $n$, etc.)
\item Cas de base de l'induction = cas de base de la récursion
\item Cas inductif: on suppose que les appels récursifs sont corrects et on en déduit que l'appel courant est correct
\item Terminaison: on montre que les appels récursifs se font sur des sous-problèmes (souvent trivial)
\end{itemize}
\end{frame}

\begin{frame}{Exemple: $\proc{Fibonacci}$}

\begin{center}\small
\fcolorbox{white}{Lightgray}{
\begin{codebox}
\Procname{$\proc{Fibonacci}(n)$}
\li \If $n \leq 1$
\li \Then \Return n \End
\li \Return $\proc{Fibonacci}(n-2)+\proc{Fibonacci}(n-1)$
\end{codebox}
}
\end{center}


Proposition: Pour tout $n$, $\proc{Fibonacci}(n)$ renvoie $F_n$.\\

Démonstration:
\begin{itemize}
\item Cas de base: pour $n=0$, $\proc{Fibonacci}(n)$ renvoie $F_0=0$. Pour $n=1$, $\proc{Fibonacci}(n)$ renvoie $F_1=1$.
\item Cas inductif:
\begin{itemize}
\item Supposons $n\geq 2$ et que pour tout $0\leq m< n$, $\proc{Fibonacci}(m)$ renvoie $F_m$.
\item Pour $n\geq 2$, $\proc{Fibonacci}(n)$ renvoie
\begin{eqnarray*}
&&\proc{Fibonacci}(n-2)+\proc{Fibonacci}(n-1)\\
&=& F_{n-2}+F_{n-1} \mbox{ (par hypothèse inductive)}\\
&=& F_n.
\end{eqnarray*}
\end{itemize}
\end{itemize}\qed

\end{frame}

\begin{frame}{Exemple: merge sort}
\begin{center}\small
\fcolorbox{white}{Lightgray}{%
    \begin{codebox}
      \Procname{$\proc{merge-sort}(A,p,r)$}
      \li \If $\id{p}<\id{r}$
      \li \Then $q \gets \lfloor \frac{p+r}{2} \rfloor$
      \li       $\proc{merge-sort}(A,p,q)$
      \li       $\proc{merge-sort}(A,q+1,r)$
      \li       $\proc{merge}(A,p,q,r)$ \End
    \end{codebox}}
\end{center}

Proposition: Pour tout $1\leq p\leq r\leq \attrib{A}{length}$, $\proc{merge-sort}(A,p,r)$ trie le sous-tableau $A[p\twodots r]$.

\bigskip

(On supposera que $\proc{Merge}$ est correct mais il faudrait le démontrer par un invariant)

\end{frame}

\begin{frame}{Exemple: merge sort}
\begin{center}\small
\fcolorbox{white}{Lightgray}{%
    \begin{codebox}
      \Procname{$\proc{merge-sort}(A,p,r)$}
      \li \If $\id{p}<\id{r}$
      \li \Then $q \gets \lfloor \frac{p+r}{2} \rfloor$
      \li       $\proc{merge-sort}(A,p,q)$
      \li       $\proc{merge-sort}(A,q+1,r)$
      \li       $\proc{merge}(A,p,q,r)$ \End
    \end{codebox}}
\end{center}

Démonstration:
\begin{itemize}
\item Cas de base: pour $r-p=0$, $\proc{merge-sort}(A,p,r)$ ne modifie pas $A$ et donc $A[p]=A[q]$ est trivialement trié
\item Cas inductif:
\begin{itemize}
\item Supposons $r-p>0$ et que pour tout $1\leq p'\leq r'\leq \attrib{A}{length}$ tels que $r'-p'<r-p$, $\proc{merge-sort}(A,p',r')$ trie $A[p'\twodots r']$
\item Les appels $\proc{merge-sort}(A,p,q)$ et $\proc{merge-sort}(A,q+1,r)$ sont corrects par hypothèse inductive (puisque $q-p<r-p$ et $r-q-1<r-p$)
\item En supposant $\proc{Merge}$ correct, $\proc{merge-sort}(A,p,r)$ est correct.
\end{itemize}
\end{itemize}\qed
\note{
$$q-p=\frac{p+r}{2}-p=\frac{r-p}{2}<(r-p)$$
$$r-q=r-\frac{p+r}{2}=\frac{r-p}{2}<(r-p)$$
}
\end{frame}

\begin{frame}{Conclusion sur la correction}

\begin{itemize}
\item Preuves de correction:
\begin{itemize}
\item Algorithmes itératifs: invariant (=induction)
\item Algorithmes récursifs: induction
\end{itemize}

\bigskip

\item Malheureusement, il n'existe pas d'outil automatique pour vérifier la correction (et la terminaison) d'algorithmes
\item Dans la suite, on ne présentera des invariants ou des preuves par induction que très sporadiquement lorsque ce sera nécessaire (cas non triviaux)
\end{itemize}

\note{Et pire que ça, on peut montrer qu'il n'est pas possible de concevoir un outil automatique pour faire ça}

\end{frame}

\section{Complexité algorithmique}

\begin{frame}{Plan}

\tableofcontents[currentsection]

\end{frame}

\subsection{Introduction}

\begin{frame}{Performance d'un algorithme}

\begin{itemize}
\item Plusieurs métriques possibles:
\begin{itemize}
\item Longueur du programme (nombre de lignes)
\item Simplicité du code
\item \alert{Espace mémoire consommé}
\item \alert{Temps de calcul}
\item ...
\end{itemize}

\bigskip

\item Les temps de calcul sont la plupart du temps utilisés
\begin{itemize}
\item Ils peuvent être quantifiés et sont faciles à comparer
\item Souvent ce qui compte réellement
\end{itemize}
\item Nous étudierons aussi l'espace mémoire consommé par nos algorithmes
\end{itemize}

\end{frame}

\begin{frame}{Comment mesurer les temps d'exécution ?}

Expérimentalement:
\begin{itemize}
\item On écrit un programme qui implémente l'algorithme et on l'exécute sur des données
\item Problèmes:
\begin{itemize}
\item Les temps de calcul vont dépendre de l'implémentation: CPU, OS, langage, compilateur, charge de la machine, OS, etc.
\item Sur quelles données tester l'algorithme ?
\end{itemize}
\end{itemize}

\begin{center}
\includegraphics[width=6cm]{Figures/cpu-fibonacci.pdf}\\
~\hfill\scriptsize(Carzaniga)
\end{center}

\note{Exemple de la recherche dans un tableau: si élément au début ou à la fin}
\end{frame}

\begin{frame}{Comment mesurer les temps d'exécution ?}

En se basant sur un modèle de calcul abstrait:
\begin{itemize}
\item Random-access machine (RAM):
\begin{itemize}
\item Types de données de base:
\begin{itemize}
\item Entiers et nombres flottants
\item Chaque mot est de taille limitée (par ex, 64 bits)
\end{itemize}
\item Opérations de base de la machine:
\begin{itemize}
\item addition, soustraction, multiplication...
\item affectation, accès à un élément d'un tableau...
\item branchement conditionnel, saut
\item appel de sous-routines, renvoi d'une valeur
\end{itemize}
\item Les opérations sont exécutées les unes après les autres (pas de parallélisme)
\item \alert{Les opérations de base prennent un temps constant}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Comment mesurer les temps d'exécution ?}
\begin{itemize}
\item Calculer les temps de calcul = sommer le temps d'exécution associé à chaque instruction du pseudo-code
\item Modèle RAM:
\begin{itemize}
\item Opérations de base: temps constant
\item Appel de sous-routines: temps de l'appel (constant) + temps de l'exécution de la sous-routine (calculé récursivement)
\end{itemize}

\bigskip

\item Le temps dépend de l'entrée (l'instance particulière du problème)
\item On étudie généralement les temps de calcul en fonction de la \alert{``taille''} de l'entrée
\begin{itemize}
\item Généralement, le nombre de valeurs pour la décrire
\item Mais ça peut être autre chose (Ex: $n$ pour $\proc{Fibonacci}$)
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Analyse du tri par insertion}
\centerline{\includegraphics[width=10cm]{Figures/02-analysisinsertionsort.pdf}}
\begin{itemize}
\item $t_j=$ nombre de fois que la condition du $\While$ est testée.
\item Temps exécution $T(n)$ (pour un tableau de taille $n$) donné par:
{\footnotesize
\begin{eqnarray*}
T(n)&=&c_1 n +c_2(n-1)+c_4(n-1)+c_5\sum_{j=2}^n t_j + c_6 \sum_{j=2}^n (t_j-1)\\
&&+c_7\sum_{j=2}^n (t_j-1)+c_8(n-1)
\end{eqnarray*}}
\end{itemize}
\end{frame}

\begin{frame}{Différents types de complexité}
\begin{itemize}
\item Même pour une taille fixée, la complexité peut dépendre de l'instance particulière
%\begin{itemize}
%\item Les valeurs de $t_j$ dépendent du tableau
%\end{itemize}
\item Soit $D_n$ l'ensemble des instances de taille $n$ d'un problème et $T(i_n)$ le temps de calcul pour une instance $i_n\in D_n$.
\item Sur quelles instances les performances d'un algorithme devraient être jugées:
\begin{itemize}
\item Cas le plus favorable (best case): $T(n)=\min\{T(i_n)| i_n\in D_n\}$
\item Cas le plus défavorable (worst case): $T(n)=\max\{T(i_n)| i_n\in D_n\}$
\item Cas moyen (average case): $T(n)=\sum_{i_n\in D_n} Pr(i_n) T(i_n)$ où $Pr(i_n)$ est la probabilité de rencontrer $i_n$
\end{itemize}
\item On se focalise généralement sur le cas \alert{le plus défavorable}
\begin{itemize}
\item Donne une borne supérieure sur le temps d'exécution.
\item Le meilleur cas n'est pas représentatif et le cas moyen est difficile à calculer.
\end{itemize}
\end{itemize}
\note{On va voir que le cas le plus favorable est tout de même intéressant dans certains contextes}
\end{frame}

\begin{frame}{Analyse du tri par insertion}
Meilleur cas:
\begin{itemize}
\item le tableau est trié $\Rightarrow t_j=1$.
\item Le temps de calcul devient:
\begin{eqnarray*}
T(n)&=&c_1 n+ c_2 (n-1)+c_4 (n-1)+c_5 (n-1)+c_8(n-1)\\
&=&(c_1+c_2+c_4+c_5+c_8)n- (c_2+c_4+c_5+c_8)
\end{eqnarray*}
\item $T(n)=an+b$ $\Rightarrow$ $T(n)$ est une fonction \alert{linéaire} de $n$\\

\end{itemize}

\end{frame}

\begin{frame}{Analyse du tri par insertion}
Pire cas:
\begin{itemize}
\item le tableau est trié par ordre décroissant $\Rightarrow t_j=j$.
\item Le temps de calcul devient:
{\footnotesize
\begin{eqnarray*}
T(n)&=&c_1 n+ c_2 (n-1)+c_4 (n-1)+c_5 \left( \frac{n(n+1)}{2}-1\right)\\
& & +c_6\left(\frac{n(n-1)}{2}\right)+c_7 \left(\frac{n(n-1)}{2}\right)+c_8 (n-1)\\
&=&(\frac{c_5}{2}+\frac{c_6}{2}+\frac{c_7}{2}) n^2 + (c_1+c_2+c_4+\frac{c_5}{2}-\frac{c_6}{2}-\frac{c_7}{2}+c_8)n\\
& & - (c_2+c_4+c_5+c_8)
\end{eqnarray*}
}
\item $T(n)=an^2+bn+c$ $\Rightarrow$ $T(n)$ est une fonction \alert{quadratique} de $n$
\end{itemize}

\end{frame}


\begin{frame}{Analyse asymptotique}
% Slide 30 intro-pas-mal (voir aussi CLRS)
\begin{itemize}
\item On s'intéresse à la vitesse de croissance (``order of growth'') de $T(n)$ lorsque $n$ croît.
\begin{itemize}
\item Tous les algorithmes sont rapides pour des petites valeurs de $n$
\end{itemize}
\item On simplifie généralement $T(n)$:
\begin{itemize}
\item en ne gardant que le terme dominant
\begin{itemize}
\item Exemple: $T(n)=10 n^3+n^2+40n+800$
\item T(1000)=100001040800, $10\cdot 1000^3=100000000000$
\end{itemize}
\item en ignorant le coefficient du terme dominant
\begin{itemize}
\item Asymptotiquement, ça n'affecte pas l'ordre relatif
\end{itemize}
\end{itemize}
\centerline{\includegraphics[width=5cm]{Figures/02-dropconstant.pdf}}
\item Exemple: Tri par insertion: $T(n)=an^2+bn+c \rightarrow n^2$.
%\item On dira que $T(n)$ croît en $n^2$ et on le notera $T(n)\in\Theta(n^2)$ ou $T(n)=\Theta(n^2)$.
\end{itemize}
\end{frame}

\begin{frame}{Pourquoi est-ce important?}

\begin{itemize}
\item Supposons qu'on puisse traiter une opération de base en $1\mu s$.
\item Temps d'exécution pour différentes valeurs de $n$

\bigskip

\begin{center}
\footnotesize
\begin{tabular}{ccccc}
\hline
T(n) & $n=10$ & $n=100$ & $n=1000$ & $n=10000$\\
\hline
$n$ & $10\mu s$ & $0.1ms$ & $1ms$ & $10ms$\\
$400n$ & $4ms$ & $40ms$ & $0.4s$ & 4$s$\\
$2n^2$ &$200\mu s$ & $20ms$ & $2s$ & 3.3$m$\\
$n^4$ &$10ms$& $100s$ & $\sim 11.5$ jours & 317 années\\
$2^n$ & $1ms$ & $4\times 10^{16}$ années & $3.4\times 10^{287}$ années & $\ldots$\\
\hline
\end{tabular}
\end{center}

\end{itemize}

~\hfill{\scriptsize(Dupont)}

\end{frame}

\begin{frame}{Pourquoi est-ce important?}

\begin{itemize}
\item Taille maximale du problème qu'on peut traiter en un temps donné:
\begin{center}
\footnotesize
\begin{tabular}{cccc}
\hline
T(n) & en 1 seconde & en 1 minute & en 1 heure\\
\hline
$n$ & $1\times 10^6$ & $6\times 10^7$ & $3.6\times 10^9$\\
$400n$ & 2500 & 150000 & $9\times 10^6$\\
$2n^2$ & 707 & 5477 & 42426\\
$n^4$ & 31 & 88 & 244\\
$2^n$ & 19 & 25 & 31\\
\hline
\end{tabular}
\end{center}

\bigskip

\item Si $m$ est la taille maximale que l'on peut traiter en un temps
  donné, que devient cette valeur si on reçoit une machine 256 fois plus puissante?
\begin{center}
\footnotesize
\begin{tabular}{cc}
\hline
T(n) &Temps\\
\hline
$n$ & $256m$\\
$400n$ & $256m$\\
$2n^2$ & $16m$\\
$n^4$ & $4m$\\
$2^n$ & $m+8$\\
\hline
\end{tabular}
\end{center}
\end{itemize}

~\hfill{\scriptsize(Dupont)}

\end{frame}

\subsection{Notations asymptotiques}

\begin{frame}{Notations asymptotiques}
\begin{itemize}
\item Permettent de caractériser le taux de croissance de fonctions $f:\nats\rightarrow \reals^+$

\bigskip

\item Trois notations:
\begin{itemize}
\item Grand-O: $f(n)\in O(g(n)) \approx f(n)\leq g(n)$
\item Grand-Omega: $f(n)\in \Omega(g(n)) \approx f(n)\geq g(n)$
\item Grand-Theta: $f(n) \in \Theta(g(n)) \approx f(n) = g(n)$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Notation grand-O}

{\small
$$O(g(n))=\{f(n)| \exists c>0, \exists n_0\geq 1\mbox{ tels que } 0\leq f(n)\leq c g(n), \forall n\geq n_0\}$$
}

\medskip

\centerline{\includegraphics[width=4cm]{Figures/02-grando.pdf}}

\medskip

\begin{itemize}
\item $f(n)\in O(g(n))\Rightarrow g(n)$ est une borne \alert{supérieure} asymptotique pour $f(n)$.
\item Par abus de notation, on écrira aussi: $f(n)=O(g(n))$.
\end{itemize}

\end{frame}

\begin{frame}{Notation grand-Omega}

{\small
$$\Omega(g(n))=\{f(n)| \exists c>0, \exists n_0\geq 1\mbox{ tels que } 0\leq c g(n)\leq f(n), \forall n\geq n_0\}$$
}

\medskip

\centerline{\includegraphics[width=4cm]{Figures/02-grandomega.pdf}}

\medskip

\begin{itemize}
\item $f(n)\in \Omega(g(n))\Rightarrow g(n)$ est une borne \alert{inférieure} asymptotique pour $f(n)$.
\item Par abus de notation, on écrira aussi: $f(n)=\Omega(g(n))$.
\end{itemize}

\end{frame}

\begin{frame}{Notation grand-Theta}

{\small
\begin{eqnarray*}
\Theta(g(n))&=&\{f(n)| \exists c_1,c_2>0, \exists n_0\geq 1\\
&&\mbox{ tels que } 0\leq c_1 g(n)\leq f(n)\leq c_2 g(n), \forall n\geq n_0\}
\end{eqnarray*}
}

\medskip

\centerline{\includegraphics[width=4cm]{Figures/02-grandtheta.pdf}}

\medskip

\begin{itemize}
\item $f(n)\in \Theta(g(n))\Rightarrow g(n)$ est une borne \alert{serrée} (``tight'') asymptotique pour $f(n)$.
\item Par abus de notation, on écrira aussi: $f(n)=\Theta(g(n))$.
\end{itemize}

\end{frame}

\begin{frame}{Exemples}
\begin{itemize}
\item $3n^5-16n+2\in O(n^5)$ ? $\in O(n)$ ? $\in O(n^{17})$ ?
\item $3n^5-16n+2\in \Omega(n^5)$ ? $\in \Omega(n)$ ? $\in \Omega(n^{17})$ ?
\item $3n^5-16n+2\in \Theta(n^5)$ ? $\in \Theta(n)$ ? $\in \Theta(n^{17})$ ?
\item $2^n+100n^6+n \in O(2^n)$ ? $\in \Theta(3^n)$ ? $\in \Omega(n^7)$ ?

\bigskip

\item Classes de complexité:
$$O(1)\subset O(\log n)\subset O(n) \subset O(n\log n) \subset O(n^{a>1})\subset O(2^n)$$
\centerline{\includegraphics[width=6cm]{Figures/02-complexclass.pdf}}
\end{itemize}

\note{$3^n$ est exponentiellement plus lent que $2^n$: $3^n/2^n=\exp(n (\log 2-\log 3))$.}

\end{frame}

\begin{frame}{Quelques propriétés}
\small
\begin{itemize}
\item $f(n)\in\Omega(g(n)) \Leftrightarrow g(n)\in O(f(n))$
\item $f(n)\in\Theta(g(n))\Leftrightarrow f(n)\in O(g(n))$ et $f(n)\in \Omega(g(n))$
\item $f(n)\in\Theta(g(n))\Leftrightarrow g(n)\in\Theta(f(n))$
%\item $f(n)\in\Theta(g(n))\Rightarrow g(n)\in O(f(n))$
\bigskip
\item Si $f(n)\in O(g(n))$, alors pour tout $k\in \nats$, on a $k\cdot f(n)\in O(g(n))$
\begin{itemize}
\item Exemple: $\log_a(n)\in O(\log_b(n))$, $a^{n+b}\in O(a^n)$
\end{itemize}
\item Si $f_1(n)\in O(g_1(n))$ et $f_2(n)\in O(g_2(n))$, alors $f_1(n)+f_2(n)\in O(g_1(n)+g_2(n))$ et $f_1(n)+f_2(n)\in O(\max\{g_1(n),g_2(n)\})$
\begin{itemize}
\item Exemple: $\sum_{i=1}^m a_i n^i\in O(n^m)$
\end{itemize}
\item Si $f_1(n)\in O(g_1(n))$ et $f_2(n)\in O(g_2(n))$, alors $f_1(n)\cdot f_2(n)\in O(g_1(n)\cdot g_2(n))$
\end{itemize}
\end{frame}

\subsection{Complexité d'algorithmes et de problèmes}

\begin{frame}{Complexité d'un algorithme}
\begin{itemize}
\item On utilise les notations asymptotiques pour caractériser la
  \alert{complexité} d'un algorithme.
\item Il faut préciser de quelle complexité on parle: générale, au
  pire cas, au meilleur cas, en moyenne...
\item La notation grand-O est de loin la plus utilisée
\begin{itemize}
\item $f(n)\in O(g(n))$ sous-entend généralement que $O(g(n))$ est le
  plus petit sous-ensemble qui contient $f(n)$ et que $g(n)$ est la plus concise possible
\item Exemple: $n^3+100n^2-n \in O(n^3)=O(n^3+n^2)\subset O(n^4) \subset O(2^n)$
\end{itemize}
\item Idéalement, les notations $O$ et $\Omega$ devraient être
  limitées au cas où on n'a pas de borne serrée.
\end{itemize}
\end{frame}

\begin{frame}{Complexité d'un algorithme}
Exemples:
\begin{itemize}
\item On dira:\\
``La complexité au pire cas du tri par insertion est $\Theta(n^2)$''\\
plutôt que\\
``La complexité au pire cas du tri par insertion est $O(n^2)$'' \\
ou ``La complexité du tri par insertion est $O(n^2)$''
\item On dira\\
``La complexité au meilleur cas du tri par insertion est $\Theta(n)$''\\
plutôt que\\
``La complexité au meilleur cas du tri par insertion est $\Omega(n)$''\\
ou ``La complexité du tri par insertion est $\Omega(n)$''
\item Par contre, on dira ``La complexité de $\proc{Fibonacci}$ est $\Omega(1.4^n)$'', car on n'a pas de borne plus précise à ce stade.
\end{itemize}
\note{Fibonacci est le bon exemple de l'utilisation de $\Omega$. On ne peut rien dire de plus à ce stade}
\end{frame}

\begin{frame}{Complexité d'un problème}
\begin{itemize}
\item Les notations asymptotiques servent aussi à caractériser la complexité
  d'un problème
\begin{itemize}
\item Un problème est $O(g(n))$  s'il existe un algorithme $O(g(n))$ pour le résoudre
\item Un problème est $\Omega(g(n))$ si tout algorithme qui le résoud est forcément $\Omega(g(n))$
\item Un problème est $\Theta(g(n))$ s'il est $O(g(n))$ et $\Omega(g(n))$
\end{itemize}
\item Exemple du problème de tri:
\begin{itemize}
\item Le problème du tri est $O(n \log n)$ (voir plus loin)
\item On peut montrer facilement que le problème du tri est $\Omega(n)$ (voir le transparent suivant)
\item On montrera plus tard que le problème de tri est en fait $\Omega(n \log n)$ et donc qu'il est $\Theta(n\log n)$.
\end{itemize}
\item Exercice: montrez que la recherche du maximum dans un tableau est $\Theta(n)$
\end{itemize}
\end{frame}


\begin{frame}{Le problème du tri est $\Omega(n)$}
Preuve par l'absurde (ou par contraposition):
\begin{itemize}
\item Supposons qu'il existe un algorithme moins que $O(n)$ pour résoudre le problème du tri
\item Cet algorithme ne peut pas parcourir tous les éléments du tableau, sinon il serait au moins $O(n)$
\item Il y a donc au moins un élément du tableau qui n'est pas vu par cet algorithme
\item Il existe donc des instances de tableau qui ne seront pas triées correctement par cet algorithme
\item Il n'y a donc pas d'algorithme plus rapide que $O(n)$ pour le tri.
\end{itemize}
\end{frame}

\subsection{Complexité d'algorithmes itératifs}

\begin{frame}{Comment calculer la complexité en pratique ?}
%\begin{itemize}
Quelques règles pour les algorithmes itératifs:
\begin{itemize}
\item Affectation, accès à un tableau, opérations arithmétiques, appel de fonction: $O(1)$
\item Instruction If-Then-Else: $O(\mbox{complexité max des deux branches})$
\item Séquence d'opérations: l'opération la plus couteuse domine (règle de la somme)
\item Boucle simple: $O(n f(n))$ si le corps de la boucle est $O(f(n))$
\end{itemize}
\end{frame}

\begin{frame}{Comment calculer la complexité en pratique ?}
%\begin{itemize}
%Quelques règles pour les algorithmes itératifs:
\begin{itemize}
\item Double boucle complète: $O(n^2 f(n))$ où $f(n)$ est la complexité du corps de la boucle
\item Boucles incrémentales: $O(n^2)$ (si corps $O(1)$)
\begin{center}\footnotesize
   \fcolorbox{white}{Lightgray}{
    \begin{codebox}
      \zi \For $i\gets 1$ \To $n$
      \zi \Do \For $j\gets 1$ \To $i$
      \zi \Do $\ldots$
      \End\End
    \end{codebox}}
\end{center}
\item Boucles avec un incrément exponentiel: $O(\log n)$ (si corps $O(1)$)
\begin{center}\footnotesize
   \fcolorbox{white}{Lightgray}{
    \begin{codebox}
      \zi $i\gets 1$
      \zi \While  $i\leq n$
      \zi \Do $\ldots$
      \zi $i\gets 2i$
      \End\End
    \end{codebox}}
\end{center}
\end{itemize}
\end{frame}

\begin{frame}{Exemple: }

$\proc{prefixAverages}(X)$:
\begin{itemize}
\item {\bf Entrée:} tableau $X$ de taille $n$
\item {\bf Sortie:} tableau $A$ de taille $n$ tel que $A[i]=\frac{\sum_{j=1}^i X[j]}{i}$
\end{itemize}

\begin{columns}
\begin{column}{5cm}
\begin{center}\small
   \fcolorbox{white}{Lightgray}{
    \begin{codebox}
      \Procname{$\proc{prefixAverages}(X)$}
      \li \For $i\gets 1$ \To $\attrib{X}{length}$
      \li \Do $a\gets 0$
      \li \For $j\gets 1$ \To $i$
      \li \Do $a\gets a+X[j]$ \End
      \li $A[i]\gets a/i$
      \End
      \li \Return A
    \end{codebox}}
\end{center}
Complexité: $\Theta(n^2)$
\end{column}
\begin{column}{5cm}
\begin{center}\small
   \fcolorbox{white}{Lightgray}{
    \begin{codebox}
      \Procname{$\proc{prefixAverages2}(X)$}
      \li $s\gets 0$
      \li \For $i\gets 1$ \To $\attrib{X}{length}$
      \li \Do $s\gets s+X[i]$
      \li $A[i]\gets s/i$
      \End
      \li \Return A
    \end{codebox}}
\end{center}
Complexité: $\Theta(n)$
\end{column}
\end{columns}
\note{interieur du for: O(1) puis O(i) puis O(1) => O(i) => on peut ne garder que le for j=1 to i. L'instruction au center est exécutée exactement...}
\end{frame}



%%%%%%%%%% a part
%% algo récursif -> complexité sous une forme récursive.
%% Exemple du merge-sort.

\subsection{Complexité d'algorithmes récursifs}

\begin{frame}{Complexité d'algorithmes récursifs}

\begin{itemize}
\item La complexité d'algorithme récursif mène généralement à une équation de récurrence
\item La résolution de cette équation n'est généralement pas triviale
\item On se contentera d'étudier quelques cas particuliers importants dans ce cours
\end{itemize}

\end{frame}

\begin{frame}{$\proc{Factorial}$ et $\proc{Fibonacci}$}
% mettre les algos et la récurrence

\begin{columns}
\begin{column}{5.5cm}
\begin{center}\footnotesize
   \fcolorbox{white}{Lightgray}{
    \begin{codebox}
\Procname{$\proc{Factorial}(n)$}
\li \If $n\isequal 0$
\li \Then \Return 1 \End
\li \Return $n \cdot \proc{Factorial}(n-1)$
    \end{codebox}}
\end{center}
{\footnotesize
\begin{eqnarray*}
T(0) & = & c_0\\
T(n) & = & T(n-1)+c_1\\
     & = & c_1 n+c_0
\end{eqnarray*}
$$\Rightarrow T(n)\in \Theta(n)$$
}
\end{column}
\begin{column}{5.5cm}
\begin{center}\footnotesize
   \fcolorbox{white}{Lightgray}{
    \begin{codebox}
      \Procname{$\proc{Fib}(n)$}
      \li \If $n \leq 1$
      \li \Then \Return n \End
      \li \Return $\proc{Fib}(n-2)+\proc{Fib}(n-1)$
    \end{codebox}}
\end{center}
{\footnotesize
\begin{eqnarray*}
T(0) & = & c_0, T(1)= c_0\\
T(n) & = & T(n-1)+T(n-2)+c_1\\
\end{eqnarray*}
}
$$\Rightarrow T(n)\in \Omega(1.4^n)$$
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Analyse du tri par fusion}

\begin{center}
\fcolorbox{white}{Lightgray}{%
    \begin{codebox}
      \Procname{$\proc{merge-sort}(A,p,r)$}
      \li \If $\id{p}<\id{r}$
      \li \Then $q \gets \lfloor \frac{p+r}{2} \rfloor$
      \li       $\proc{merge-sort}(A,p,q)$
      \li       $\proc{merge-sort}(A,q+1,r)$
      \li       $\proc{merge}(A,p,q,r)$ \End
    \end{codebox}}
\end{center}

\begin{itemize}
\item Récurrence:
\begin{columns}
\begin{column}{5cm}
\begin{eqnarray*}
T(1) & = & c_1\\
T(n) & = & 2 T(n/2)+c_2 n+c_3\\
\end{eqnarray*}
\end{column}
\begin{column}{5cm}
\begin{eqnarray*}
T(1) & = & \Theta(1)\\
T(n) & = & 2 T(n/2)+\Theta(n)\\
\end{eqnarray*}
\end{column}
\end{columns}
\end{itemize}
\note{Dire que celle-ci est importante}
\end{frame}

\begin{frame}{Analyse du tri par fusion}
\begin{columns}
\begin{column}{6cm}
\begin{itemize}
\item Simplifions la récurrence en:
\begin{eqnarray*}
T(1) & = & c\\
T(n) & = & 2 T(n/2)+c n
\end{eqnarray*}
\item On peut représenter la récurrence par un arbre de récursion
\item La complexité est la somme du coût de chaque noeud
\end{itemize}
\end{column}
\begin{column}{5.5cm}
\centerline{\includegraphics[width=5.5cm]{Figures/02-recurtree-merge.pdf}}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Analyse du tri par fusion}
\begin{columns}
\begin{column}{5cm}
\begin{itemize}
\item Chaque niveau a un coût $cn$
\item En supposant que $n$ est une puissance de 2, il y a $\log_2 n+1$ niveaux
\item Le coût total est $cn\log_2 n+cn \in \Theta(n\log n)$
\end{itemize}
\end{column}
\begin{column}{7cm}
\centerline{\includegraphics[width=6.5cm]{Figures/02-recurtree-merge-annote.pdf}}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Remarques}
Limitations de l'analyse asymptotique
\begin{itemize}
\item Les facteurs constants ont de l'importance pour des problèmes de petite taille
\begin{itemize}
\item Le tri par insertion est plus rapide que le tri par fusion pour $n$ petit
\end{itemize}
\item Deux algorithmes de même complexité (grand-O) peuvent avoir des propriétés très différentes
\begin{itemize}
\item Le tri par insertion est en pratique beaucoup plus efficace que
  le tri par sélection sur des tableaux presque triés
\end{itemize}
\end{itemize}

\bigskip

Complexité en espace
\begin{itemize}
\item S'étudie de la même manière, avec les mêmes notations
\item Elle est bornée par la complexité en temps (pourquoi ?)
\end{itemize}

\end{frame}

\begin{frame}{Ce qu'on a vu}

\begin{itemize}
\item Correction d'algorithmes itératifs (par invariant) et récursifs (par induction)
\item Notions de complexité algorithmique
\item Notations asymptotiques
\item Calcul de complexité d'algorithmes itératifs et récursifs
\end{itemize}

\end{frame}

%A la fin de chaque cours, je vais dire toutes les simplifications
%qu'on a prise et encourager les étudiants à se poser certaines
%questions. Exemple:

