
\part{Dictionnaires}

% Superbe site web avec les differents algorithmes

% http://www.sorting-algorithms.com/


% Plan

% recherche séquentielle
% recherche binaire (dichotomique) -> problème insertion reste lente
% binary search trees -> insertion et recherche rapide mais seulement en moyenne
% RB trees -> insertion et recherche rapide dans le pire cas
% table à accès direct: tout en $O(1)$ mais prend beaucoup de mémoire
% table hash: tout en ordre $O(1)$ mais structure non dynamique

\begin{frame}{Plan}

\tableofcontents[hideallsubsections]

\end{frame}

\section{Introduction}

\begin{frame}{Dictionnaires}
\begin{itemize}
\item Définition: un \alert{dictionnaire} est un ensemble dynamique
  d'objets avec des clés comparables qui supportent les opérations
  suivantes:
\begin{itemize}
\item $\proc{Search}(S,k)$ retourne un pointeur $x$ vers un élément dans $S$ tel que $\attrib{x}{key}=k$, ou $\const{NIL}$ si un tel élément n'appartient pas à $S$.
\medskip
\item $\proc{Insert}(S,x)$ insère l'élément $x$ dans le dictionnaire $S$. Si un élément de même clé se trouve déjà dans le dictionnaire, on met à jour sa valeur
\medskip
\item $\proc{Delete}(S,x)$ retire l'élément $x$ de $S$. Ne fait rien si l'élément n'est pas dans le dictionnaire.
\end{itemize}
\medskip
\item Pour faciliter la recherche, on peut supposer qu'il existe un ordre total sur les clés.
\end{itemize}
\end{frame}

\begin{frame}{Dictionnaires}
\begin{itemize}
\item Deux objectifs en général:
\begin{itemize}
\item minimiser le coût pour l'insertion et l'accès au données
\item minimiser l'espace mémoire pour le stockage des données
\end{itemize}
\item Exemples d'applications:
\begin{itemize}
\item Table de symboles dans un compilateur
\item Table de routage d'un DNS
\item \ldots
\end{itemize}
\item Beaucoup d'implémentations possibles
\end{itemize}
\note{Ici:
\begin{itemize}
\item Pas d'ordre sur les clés (du moins, on ne l'exploite pas)
\begin{itemize}
\item tableau à accès direct
\item table de hachage
\end{itemize}
\item Ordre sur les clés
\begin{itemize}
\item tableau trié
\item arbre binaire de recherche
\end{itemize}
\end{itemize}
}
\end{frame}

\begin{frame}{Liste liée}
Première solution:
\begin{itemize}
\item On stocke les paires clé-valeur dans une liste liée
\item Recherche:
\begin{center}
{\footnotesize
\fcolorbox{white}{Lightgray}{%
      \begin{codebox}
        \Procname{$\proc{List-Search}(L,k)$}
        \li $x\gets \attrib{L}{head}$
        \li \While $x\ne \const{NIL}\wedge\attrib{x}{key}\ne k$
        \li \Do $x\gets \attrib{x}{next}$
            \End
        \li \Return $x$
      \end{codebox}}
%% \fcolorbox{white}{Lightgray}{%
%%       \begin{codebox}
%%         \Procname{$\proc{List-Delete}(L,x)$}
%%         \li \If $\attrib{x}{prev}\ne \const{NIL}$
%%         \li \Then $\attrib{x}{prev}.\id{next}\gets \attrib{x}{next}$
%%         \li \Else $\attrib{L}{head}\gets \attrib{x}{next}$\End
%%         \li \If $\attrib{x}{next}\ne \const{NIL}$
%%         \li \Then $\attrib{x}{next}.\id{prev}\gets \attrib{x}{prev}$ \End
%%       \end{codebox}}
}
\end{center}
\item Insertion (resp. Suppression)
\begin{itemize}
\item On recherche la clé dans la liste
\item Si elle existe, on remplace la valeur (resp. on la supprime)
\item Si elle n'existe pas, on la place en début de liste (resp. on ne fait rien)
\end{itemize}
\item Complexité au pire cas\hfill{\it (meilleur cas ?)}
% remplacer ça par une table
\begin{itemize}
\item Insertion: $O(N)$
\item Recherche: $O(N)$
\item Suppression: $O(N)$
\end{itemize}
%\item Peut-on améliorer la recherche ?
\end{itemize}
\end{frame}

\begin{frame}{Vecteur trié}

Deuxième solution:
\begin{itemize}
\item On suppose qu'il existe un ordre total sur les clés
\item On stocke les éléments dans un \alert{vecteur} qu'on maintient trié
\item Recherche dichotomique (approche ``diviser-pour-régner'')

\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Binary-Search}(V,k,low,high)$}
    \li \If $low>high$
    \li \Then \Return \const{NIL} \End
    \li $mid\gets \lfloor (low+high)/2\rfloor$
    \li $x\gets \proc{Elem-At-Rank}(V,mid)$
    \li \If $k\isequal x.key$
    \li \Then \Return $x$
    \li \ElseIf $k>x.key$
    \li \Then \Return $\proc{Binary-Search}(V,k,mid+1,high)$
    \li \Else \Return $\proc{Binary-Search}(V,k,low,mid-1)$
    \End
\end{codebox}}
\end{small}
\end{center}
Complexité: $O(\log n)$
\end{itemize}
\note{Comment est-ce que vous feriez l'insertion? comme insertion sort ?}
\end{frame}

\begin{frame}{Vecteur trié}
\begin{itemize}
\item Insertion: recherche de la position par $\proc{Binary-Search}$ puis insertion dans le vecteur par $\proc{Insert-At-Rank}$ (=décalage des éléments vers la droite).
\item Suppression: recherche puis suppression par $\proc{Remove-At-Rank}$ (=décalage des éléments vers la gauche).
\item Complexité au pire cas\hfill{\it (meilleur cas ?)}
\begin{itemize}
\item Insertion: $O(N)$ (on doit décaler les éléments à droite de la clé)
\item Recherche: $O(\log N)$ (recherche dichotomique)
\item Suppression: $O(N)$ (on doit décaler les éléments à gauche de la clé)
\end{itemize}
(Si le vecteur est implémenté par un tableau extensible !)
\end{itemize}

\note{Est-ce que ça ne vous donne pas une autre idée d'algorithme de tri ? Binary insertion sort: complexité ? Intéressant si le coût d'une comparaison est plus grand que le coût d'un swap: example si la comparaison nécessite de faire un calcul couteux (log(n) comparaison dans un cas, n dans l'aute)}

\end{frame}

\begin{frame}{Dictionnaires: jusqu'ici}

  \begin{center}\small
    \def\arraystretch{1.5}\renewcommand{\tabcolsep}{1mm}
    \begin{tabular}{@{}lcccccc@{}}
    &\multicolumn{3}{c}{\emph{Pire cas}} & \multicolumn{3}{c}{\emph{En moyenne}}\\
    \emph{Implémentation}& \proc{Search} & \proc{Insert} & \proc{Delete} & \proc{Search} & \proc{Insert} & \proc{Delete}\\
    \hline\hline
    Liste &$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$\\
    \hline
    Vecteur trié&$\Theta(\log n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(\log n)$&$\Theta(n)$&$\Theta(n)$\\
    \hline\hline
  \end{tabular}
  \end{center}

\bigskip

Peut-on obtenir à la fois une insertion et une recherche ``efficaces''?

\end{frame}


\section{Arbres binaires de recherche}

\begin{frame}{Plan}

\tableofcontents[currentsection]

\end{frame}

\subsection{Type de données abstrait pour un arbre}

\begin{frame}{Type de données abstrait pour un arbre}
\begin{itemize}
\item Principe:
\begin{itemize}
\item Des données sont associées aux n\oe uds d'un arbre
\item Les n\oe uds sont accessibles les uns par rapport aux autres selon leur position dans l'arbre
\end{itemize}
\item Interface: Pour un arbre $T$ et un n\oe ud $n$
\begin{itemize}
\item $\proc{Parent}(T,n)$: renvoie le parent d'un n\oe ud $n$ (signale une erreur si $n$ est la racine)
\item $\proc{Children}(T,n)$: renvoie une structure de données (ordonnée
  ou non) contenant les fils du n\oe ud $n$ (exemple: une liste)
\item $\proc{isRoot}(T,n)$: renvoie vrai si $n$ est la racine de l'arbre
\item $\proc{isInternal}(T,n)$: renvoie vrai si $n$ est un n\oe ud interne
\item $\proc{isExternal}(T,n)$: renvoie vrai si $n$ est un n\oe ud externe
\item $\proc{GetData}(T,n)$: renvoie les données associées au n\oe ud $n$
\item $\proc{Left}(T,n)$, $\proc{Right}(T,n)$: renvoie les fils gauche et droit de $n$ (pour un arbre binaire)
\item $\proc{Root}(T)$: renvoie le n\oe ud racine de l'arbre
\item $\proc{Size}(T)$: renvoie le nombre de n\oe uds de l'arbre
\item \ldots
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Exemples d'opération sur un arbre}
\begin{itemize}
\item Calcul de la profondeur d'un n\oe ud

\begin{center}\small
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Depth-rec}(T,n)$}
    \li \If $\proc{isRoot}(T,n)$
    \li \Then \Return 0\End
    \li \Return $1+\proc{Depth-rec}(T,\proc{Parent}(T,n))$
\end{codebox}}
\end{center}
\item Version itérative

\begin{center}\small
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Depth-iter}(T,n)$}
    \li $d\gets 0$
    \li \While \textbf{not} $\proc{isRoot}(T,n)$ \Do
    \li $d\gets d+1$
    \li $n\gets \proc{Parent}(T,n)$ \End
    \li \Return $d$
\end{codebox}}
\end{center}
\item Complexité en temps: $O(n)$, où $n$ est la taille de l'arbre (si
  les opérations de l'interface sont $O(1)$)
\end{itemize}
\note{Complexité en espace: $O(n)$ pour la version récursive}
\end{frame}

\begin{frame}{Exemples d'opération sur un arbre}
\begin{itemize}
\item Calcul de la hauteur de l'arbre

\begin{center}\small
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Height}(T,n)$}
    \li \If $\proc{isExternal}(T,n)$
    \li \Then \Return 0 \End
    \li $h\gets 0$
    \li \For \textbf{each} $n2$ \textbf{in} $\proc{Children}(T,n)$
    \li \Do $h\gets \max(h,\proc{Height}(T,n2))$\End
    \li \Return $h+1$
\end{codebox}}
\end{center}

\item Complexité en temps: $O(n)$, où $n$ est la taille de l'arbre (si
  les opérations de l'interface sont $O(1)$)
\end{itemize}

\end{frame}

\begin{frame}{Implémentation d'un arbre binaire}

Première solution: \alert{numérotation de niveaux}
\begin{itemize}
\item L'arbre est représenté par un vecteur (ou un tableau)
\item Chaque position dans l'arbre est associée à un rang particulier:
\begin{itemize}
\item La racine est en position 1
\item Si un n\oe ud est au rang $r$, son successeur gauche est au rang $2r$, son successeur droit au rang $2r+1$
\end{itemize}
\item Si l'arbre binaire n'est pas un arbre binaire complet, le vecteur contiendra des trous (qu'il faudra pouvoir identifier)
\item Complexité en temps des opérations: $O(1)$
\item Complexité en espace: $O(2^n)$ pour un arbre de $n$ n\oe uds ($\Theta(n)$ pour un arbre binaire complet)
\end{itemize}

\note{Au pire, l'arbre aura une profondeur $h=n$, et donc il faudra pouvoir référencer $O(2^h)=O(2^n)$ éléments

\bigskip

On peut représenter un arbre k-aire avec le même truc. kr, kr+1, etc. pour les fils. Suppose qu'on connaisse le kmax}

\end{frame}

\begin{frame}{Implémentation d'un arbre binaire}

Deuxième solution: \alert{structure liée}
\begin{itemize}
\item Principe: on retient pour chaque n\oe ud $n$ de l'arbre:
\begin{itemize}
\item Un champ de données ($\attrib{n}{data}$)
\item Un pointeur vers son n\oe ud parent ($\attrib{n}{parent}$)
\item Un pointeur vers ses fils gauche et droit ($\attrib{n}{left}$ et $\attrib{n}{right}$)
\end{itemize}
\item Complexité en temps des opérations: $O(1)$
\item Complexité en espace: $\Theta(n)$ pour $n$ n\oe uds
\end{itemize}

\bigskip

{\it (généralise la notion de liste liée)}

\note{Représentation par un tableau (left, right, parent, etc.). Avantage: plus compact mais nécessite un tableau de taille fixée.}
\end{frame}

\begin{frame}{Implémentation d'un arbre quelconque}

\begin{itemize}
\item Même structure liée que pour un arbre binaire
\item Mais on remplace $\attrib{n}{left}$ et $\attrib{n}{right}$ par un pointeur $\attrib{n}{children}$ vers un ensemble dynamique
\item Le type d'ensemble dynamique (vecteur, liste, \ldots) dépendra des opérations devant être effectuées
\end{itemize}

\centerline{\includegraphics[width=9cm]{Figures/05-arbrequelconque.pdf}}

{\it (on peut aussi représenter un arbre quelconque par un arbre binaire)}
\note{Représentation fils-gauche, frère-droit}
\end{frame}

\begin{frame}{Implémentation des arbres binaires}

Implémentation des arbres binaires dans le reste de ce cours:
\begin{columns}
\begin{column}{7.5cm}
\begin{itemize}
\item $T$ représente l'arbre, qui consiste en un ensemble de n\oe uds
\item $T.root$ est le n\oe ud racine de l'arbre $T$
\item N\oe ud $x$
\begin{itemize}
\item $\attrib{x}{parent}$ est le parent du n\oe ud $x$
\item $\attrib{x}{key}$ est la clé stockée au n\oe ud $x$
\item $\attrib{x}{left}$ est le fils de  gauche du n\oe ud $x$
\item $\attrib{x}{right}$ est le fils de droite du n\oe ud $x$
\end{itemize}
\end{itemize}
\end{column}
\begin{column}{4cm}
\begin{center}
\includegraphics[width=3cm]{Figures/05-bstnode.pdf}
\end{center}
\end{column}
\end{columns}

\bigskip
Pour simplifier les notations, nos fonctions seront implémentées directement sur base de cette implémentation (et pas de l'interface générale)

\note{Le passage à une autre implémentation devrait être relativement aisé}

\end{frame}

\begin{frame}{Parcours d'arbres (binaire)}

\begin{itemize}
\item Un parcours d'arbre est une façon d'\alert{ordonner} les n\oe uds d'un arbre afin de les parcourir
\item Différents types de parcours:
\begin{itemize}
\item Parcours en profondeur:
\begin{itemize}
\item Infixe (en ordre)
\item Préfixe (en préordre)
\item Suffixe (en postordre)
\end{itemize}
\item Parcours en largeur
\end{itemize}
\end{itemize}
\note{Parcours en largeur d'un tas ? $\Rightarrow$ on passe simplement séquentiellement sur les éléments}
\end{frame}

\begin{frame}{Parcours infixe}

\begin{center}
\includegraphics[width=5cm]{Figures/05-onebst2.pdf}
\bigskip

$\Rightarrow \langle A, B, D, F, H, K\rangle$
\end{center}

\begin{itemize}
\item Parcours infixe (en ordre): Chaque n\oe ud est visité \alert{après} son fils gauche et \alert{avant} son fils droit

\bigskip
\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Inorder-Tree-Walk}(x)$}
    \li \If $x\neq \const{NIL}$
    \li \Then $\proc{Inorder-Tree-Walk}(\attrib{x}{left})$
    \li print $\attrib{x}{key}$
    \li $\proc{Inorder-Tree-Walk}(\attrib{x}{right})$
    \End
\end{codebox}}
\end{small}
\end{center}

\end{itemize}

\end{frame}

\begin{frame}{Parcours préfixe}

\begin{center}
\includegraphics[width=6cm]{Figures/05-onebst2.pdf}
\bigskip

$\Rightarrow \langle F, B, A, D, H, K\rangle$
\end{center}

\begin{itemize}
\item Parcours préfixe (en préordre): chaque n\oe ud est visité \alert{avant} ses fils

\bigskip
\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Preorder-Tree-Walk}(x)$}
    \li \If $x\neq \const{NIL}$
    \li \Then print $\attrib{x}{key}$
    \li $\proc{Preorder-Tree-Walk}(\attrib{x}{left})$
    \li $\proc{Preorder-Tree-Walk}(\attrib{x}{right})$
    \End
\end{codebox}}
\end{small}
\end{center}

\end{itemize}

\end{frame}

\begin{frame}{Parcours postfixe}

\begin{center}
\includegraphics[width=6cm]{Figures/05-onebst2.pdf}
\bigskip

$\Rightarrow \langle A, D, B, K, H, F\rangle$
\end{center}

\begin{itemize}
\item Parcours postfixe (en postordre): chaque n\oe ud est visité \alert{après} ses fils

\bigskip
\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Postorder-Tree-Walk}(x)$}
    \li \If $x\neq \const{NIL}$
    \li \Then $\proc{Postorder-Tree-Walk}(\attrib{x}{left})$
    \li $\proc{Postorder-Tree-Walk}(\attrib{x}{right})$
    \li print $\attrib{x}{key}$
    \End
\end{codebox}}
\end{small}
\end{center}

\end{itemize}

\end{frame}

\begin{frame}{Complexité des parcours}


Tous les parcours en profondeur sont $\Theta(n)$ en temps
\begin{itemize}
\item Soit $T(n)$ le nombre d'opérations pour un arbre avec $n$ n\oe uds
\item On a $T(n)=\Omega(n)$ (on doit au moins parcourir chaque n\oe ud).
\item Etant donné la récurrence, on a:
$$T(n)\leq T(n_L)+T(n-n_L-1) + d$$ où $n_L$ est le nombre de n\oe uds du sous-arbre à gauche et $d$ une constante
\item On peut prouver par induction que $T(n)< (c+d) n +c$ où $c=T(0)$.
\item $T(n)=\Omega(n)$ et $T(n)=O(n)$ $\Rightarrow$ $T(n)=\Theta(n)$
\end{itemize}

\note{Preuve par induction:
\centerline{\includegraphics[width=10cm]{Figures/05-proofinorder.pdf}}
}
\end{frame}

\begin{frame}{Parcours en largeur}

\begin{itemize}
\item Parcours en largeur: on visite le n\oe ud le plus proche de la racine qui n'a pas déjà été visité. Correspond à une visite de n\oe ud de profondeur 1, puis 2, \ldots.
\item Implémentation à l'aide d'une file en $\Theta(n)$
\end{itemize}

%\bigskip

\begin{columns}
\begin{column}{5cm}
\begin{center}
\includegraphics[width=6cm]{Figures/05-onebst2.pdf}
\bigskip

$\Rightarrow \langle F,B,H,A,D,K\rangle$
\end{center}
\end{column}~~~~~~~
\begin{column}{5cm}
\begin{center}
\begin{footnotesize}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Breadth-Tree-Walk}(x)$}
    \li $Q\gets$''Empty queue''
    \li $\proc{Enqueue}(Q,x)$
    \li \While \textbf{not} $\proc{Queue-Empty}(Q)$
    \li \Do $y\gets\proc{Dequeue}(Q)$
    \li print $y.key$
    \li \If $y.left\neq \const{NIL}$
    \li \Then  $\proc{Enqueue}(Q,y.left)$ \End
    \li \If $y.right\neq \const{NIL}$
    \li \Then  $\proc{Enqueue}(Q,y.right)$ \End
    \End
\end{codebox}}
\end{footnotesize}
\end{center}
\end{column}
\end{columns}

\medskip

\emph{(Exercice: Implémenter les parcours en profondeur de manière non récursive)}
\note{Solution: il faut utiliser une pile mais une solution simple peut être trouvée avec une comparaison de pointeurs (voir les solutions du bouquin)

\bigskip

Quid de l'espace: taille max de la file ?? ($O(n/2)$)

}
\end{frame}

\subsection{Arbre binaire de recherche}

\begin{frame}{Plan}

\tableofcontents[currentsection]

\end{frame}


\begin{frame}{Arbres binaires de recherche}

\begin{itemize}
\item Une structure d'arbre binaire implémentant un dictionnaire, avec
  des opérations en $O(h)$ où $h$ est la hauteur de l'arbre


\bigskip

\item Chaque n\oe ud de l'arbre binaire est associé à une clé
\item L'arbre satisfait à la propriété d'arbre binaire de recherche
\begin{itemize}
\item Soient deux n\oe uds $x$ et $y$.
\item Si $y$ est dans le sous-arbre de gauche de $x$, alors $y.key<x.key$
\item Si $y$ est dans le sous-arbre de droite de $x$, alors $y.key\geq x.key$
\end{itemize}
\end{itemize}

\centerline{\includegraphics[width=9cm]{Figures/05-arbresbinaires.pdf}}

\note{Structure qui implémente la recherche binaire qu'on a vue la semaine passée}
\end{frame}

\begin{frame}{Parcours d'un arbre binaire de recherche}

\begin{center}
\includegraphics[width=5cm]{Figures/05-onebst.pdf}
\bigskip

$\Rightarrow \langle 2, 5, 5, 6, 7, 8\rangle$
\end{center}

\begin{itemize}
\item Le parcours infixe d'un arbre binaire de recherche permet d'afficher les clés par ordre croissant

\bigskip
\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Inorder-Tree-Walk}(x)$}
    \li \If $x\neq \const{NIL}$
    \li \Then $\proc{Inorder-Tree-Walk}(\attrib{x}{left})$
    \li print $\attrib{x}{key}$
    \li $\proc{Inorder-Tree-Walk}(\attrib{x}{right})$
    \End
\end{codebox}}
\end{small}
\end{center}

\end{itemize}

\end{frame}

\begin{frame}{Recherche dans un arbre binaire}
\begin{itemize}
\item Recherche binaire
\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Tree-Search}(x,k)$}
    \li \If $x\isequal \const{NIL}$ or $k\isequal \attrib{x}{key}$
    \li \Then \Return $x$\End
    \li \If $k<\attrib{x}{key}$
    \li \Then \Return $\proc{Tree-Search}(\attrib{x}{left},k)$
    \li \Else \Return $\proc{Tree-Search}(\attrib{x}{right},k)$
\end{codebox}}

\medskip

Appel initial (à partir d'un arbre $T$)\\
\fcolorbox{white}{Lightgray}{%
$\proc{Tree-Search}(\attrib{T}{root},k)$
}
\end{small}
\end{center}

\bigskip

\item Complexité ? $T(n)=O(h)$, où $h$ est la hauteur de l'arbre
\item Pire cas: $h=n$
\end{itemize}

\end{frame}

\begin{frame}{Recherche dans un arbre binaire}
\begin{itemize}
\item $\proc{Tree-Search}$ est récursive terminale.
\item Version itérative
\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Iterative-Tree-Search}(T,k)$}
    \li $x\gets \attrib{T}{root}$
    \li \While $x\neq \const{NIL}$ and $k\neq \attrib{x}{key}$
    \li \Do \If $k<\attrib{x}{key}$
    \li \Then $x\gets \attrib{x}{left}$
    \li \Else $x\gets \attrib{x}{right}$
    \End\End
    \li \Return x
\end{codebox}}
\end{small}
\end{center}

\bigskip

\end{itemize}
\note{Invariant: Si $k$ est dans l'arbre, elle se trouve dans le sous-arbre dont la racine est $x$

\bigskip

Faire le lien avec la recherche binaire}

\end{frame}

\begin{frame}{Clés maximale et minimale}
\begin{itemize}
\item Etant donné la propriété d'arbre binaire
\begin{itemize}
\item La clé minimale se trouve dans le n\oe ud le plus à gauche
\item La clé maximale se trouve dans le no\oe ud le plus à droite
\end{itemize}

\bigskip

\begin{center}
\fcolorbox{white}{Lightgray}{%
\begin{codebox}
          \Procname{$\proc{Tree-Minimum}(x)$}
          \li \While $\attrib{x}{left}\ne\const{NIL}$
          \li \Do $x\gets\attrib{x}{left}$
              \End
          \li \Return $x$
        \end{codebox}}~~~~~\fcolorbox{white}{Lightgray}{%
\begin{codebox}
          \Procname{$\proc{Tree-Maximum}(x)$}
          \li \While $\attrib{x}{right}\ne\const{NIL}$
          \li \Do $x\gets\attrib{x}{right}$
              \End
          \li \Return $x$
        \end{codebox}}
\end{center}

\bigskip

\item Complexité: $O(h)$, où $h$ est la hauteur de l'arbre.
\end{itemize}
\end{frame}

\begin{frame}{Successeur et prédécesseur}
\begin{itemize}
\item Etant donné un n\oe ud $x$, trouver le n\oe ud contenant la valeur de clé suivante (dans l'ordre)

\begin{center}
\includegraphics[width=7cm]{Figures/05-treesuccessor.pdf}

\bigskip

Ex: successeur de 15 $\rightarrow 17$, successeur de 4 $\rightarrow 6$.
\end{center}


\item Le successeur de $x$ est le minimum du sous-arbre de droite s'il existe
\item Sinon, c'est le premier ancêtre $a$ de $x$ tel que $x$ tombe dans le sous-arbre de gauche de $a$.
\end{itemize}
\end{frame}

\begin{frame}{Successeur et prédécesseur}

\begin{columns}
\begin{column}{5cm}
\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{
\begin{codebox}
      \Procname{$\proc{Tree-Successor}(x)$}
      \li \If $\attrib{x}{right}\ne\const{NIL}$\Then
      \li \Return $\proc{Tree-Minimum}(\attrib{x}{right})$\End
      \li $y\gets\attrib{x}{parent}$
      \li \While $y\ne\const{NIL}$ and $x\isequal\attrib{y}{right}$
      \li \Do $x\gets y$
      \li     $y\gets\attrib{y}{parent}$
      \End
      \li \Return $y$
    \end{codebox}}
\end{small}
\end{center}
\end{column}
\begin{column}{5cm}
\begin{center}

  \bigskip

\bigskip

\includegraphics[width=5cm]{Figures/05-treesuccessor.pdf}
\end{center}
\end{column}
\end{columns}

\bigskip

Complexité: $O(h)$, où $h$ est la hauteur de l'arbre

\bigskip

\emph{(Exercice: $\proc{Tree-Predecessor}$)}

\note{Attention, algo assez tordu}

\end{frame}

\begin{frame}{Insertion}

\begin{center}
\includegraphics[width=8cm]{Figures/05-bstinsertion.pdf}
\end{center}

\begin{itemize}
\item Pour insérer $x$, on recherche la clé $\attrib{x}{key}$ dans l'arbre
\item Si on ne la trouve pas, on l'ajoute à l'endroit où la recherche s'est arrêtée.
\end{itemize}
\end{frame}

\begin{frame}{Insertion}

  \begin{columns}
    \begin{column}{5cm}
\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{
      \begin{codebox}
        \Procname{$\proc{Tree-Insert}(T, z)$}
      \li $y\gets\const{NIL}$
      \li $x\gets\attrib{T}{root}$
      \li \While $x\ne\const{nil}$
      \li \Do $y\gets x$
      \li     \If $\attrib{z}{key}<\attrib{x}{key}$
      \li     \Then $x\gets\attrib{x}{left}$
      \li     \Else $x\gets\attrib{x}{right}$
      \End
      \End
      \li $\attrib{z}{parent}\gets y$
      \li \If $y\isequal \const{NIL}$
      \li \Then \Comment Tree $T$ was empty
      \li        $\attrib{T}{root}\gets z$
      \li \ElseIf $\attrib{z}{key}<\attrib{y}{key}$
      \li       \Then $\attrib{y}{left}\gets z$
      \li       \Else $\attrib{y}{right}\gets z$
      \End
      \End
      \end{codebox}}
\end{small}
\end{center}
    \end{column}
    \begin{column}{5cm}
      \begin{center}
      \includegraphics[width=5cm]{Figures/05-bstinsertion.pdf}
      \end{center}

\bigskip

\bigskip

Complexité: $O(h)$ où $h$ est la hauteur de l'arbre
    \end{column}
  \end{columns}

\note{On suppose qu'on insère deux fois une clé qui y serait déjà. Sinon, on peut d'abord la chercher. Si elle est là, on update, sinon, on insère. $x$ trace le chemin, $y$ maintient le pointeur vers le parent de $x$.\\

\bigskip


\centerline{\includegraphics[width=8cm]{Figures/05-deletebstexample.pdf}}
}

\end{frame}

\begin{frame}{Suppression}

3 cas à considérer en fonction du n\oe ud $z$ à supprimer:
\begin{itemize}
\item $z$ n'a pas de fils gauche: remplacer $z$ par son fils droit
\centerline{\includegraphics[width=5cm]{Figures/05-bstdelete-1.pdf}}
\item $z$ n'a pas de fils droit: remplacer $z$ par son fils gauche
\centerline{\includegraphics[width=5cm]{Figures/05-bstdelete-2.pdf}}
\end{itemize}

\note{On pourrait aussi ne rien faire et garder les n\oe uds dans la
  structure et les marquer. Problème: beaucoup de place pour rien

\bigskip

Virer le minimum et le maximum est facile.



}

\end{frame}

\begin{frame}
\begin{itemize}
\item $z$ a deux fils: rechercher le successeur $y$ de $z$.\\\emph{NB: $y$ est dans le sous-arbre de droite et n'a pas de fils gauche.}
\begin{itemize}
\item Si $y$ est le fils droit de $z$, remplacer $z$ par $y$ et conserver le fils droit de $y$
\centerline{\includegraphics[width=5.5cm]{Figures/05-bstdelete-3.pdf}}
\item Sinon, $y$ est dans le sous-arbre droit de $z$ mais n'en est pas la racine. On remplace $y$ par son propre fils droit et on remplace $z$ par $y$.
\centerline{\includegraphics[width=8.5cm]{Figures/05-bstdelete-4.pdf}}
\end{itemize}
\end{itemize}
\note{$y$ n'a pas de fils gauche sinon, le successeur de $z$ se trouverait dans le sous-arbre de gauche qui correspond à des valeurs plus petites que $y$ et plus grande que $z$.
~\\
}
\end{frame}

\begin{frame}{Suppression}
\vspace{-1cm}
\begin{columns}
\begin{column}{6cm}
\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Tree-Delete}(T,z)$}
    \li \If $\attrib{z}{left}\isequal \const{NIL}$
    \li \Then $\proc{Transplant}(T,z,z.right)$
    \li \ElseIf $\attrib{z}{right}\isequal\const{NIL}$
    \li \Then $\proc{Transplant}(T,z,z.left)$
    \li \Else \Comment $z$ has two children
    \li $y\gets\proc{Tree-Successor}(z)$
    \li \If $\attrib{y}{parent}\ne z$
    \li \Then $\proc{Transplant}(T,y,y.right)$
    \li $\attrib{y}{right}\gets\attrib{z}{right}$
    \li $\attrib{y}{right}.\id{parent} \gets y$
    \End
    \li \Comment Replace $z$ by $y$
    \li $\proc{Transplant}(T,z,y)$
    \li $\attrib{y}{left}\gets \attrib{z}{left}$
    \li $\attrib{y}{left}.\id{parent}\gets y$
    \End
\end{codebox}}
\end{small}
\end{center}
\end{column}
\begin{column}{6cm}

\bigskip

\bigskip

\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Transplant}(T,u,v)$}
    \li\If $\attrib{u}{parent}\isequal \const{NIL}$
    \li\Then $\attrib{T}{root}\gets v$
    \li \ElseIf $u\isequal \attrib{u}{parent}.\id{left}$
    \li \Then $\attrib{u}{parent}.\id{left}\gets v$
    \li \Else $\attrib{u}{parent}.\id{right}\gets v$
    \End
    \li \If $v\ne\const{NIL}$
    \li \Then $\attrib{v}{parent}=\attrib{u}{parent}$
    \End
  \end{codebox}}
\end{small}
\end{center}

\end{column}
\end{columns}

Complexité: $O(h)$ pour un arbre de hauteur $h$\\(Tout est $O(1)$ sauf l'appel à $\proc{Tree-Successor}$).

\note{Transplant: remplace $u$ par $v$ dans $T$. On vérifie qu'il se trouve à gauche ou à la droite de son parent.\\
\bigskip

Pourquoi pas avec le prédécesseur ??? (Parce qu'on peut avoir la même clé que $z$ à droite ($\geq$)}
\end{frame}

\begin{frame}{Arbres binaires de recherche}

\begin{itemize}
\item Toutes les opérations sont $O(h)$ où $h$ est la hauteur de l'arbre
\item Si $n$ éléments ont été insérés dans l'arbre:
\begin{itemize}
\item Au pire, $h=n-1=O(n)$
\begin{itemize}
\item Elements insérés en ordre
\end{itemize}
\item Au mieux, $h=\lceil\log_2 n\rceil=O(\log n)$
\begin{itemize}
\item Pour un arbre binaire complet
\end{itemize}
\item En moyenne, on peut montrer que $h=O(\log n)$
\begin{itemize}
\item En supposant que les éléments ont été insérés en ordre aléatoire
\end{itemize}
\end{itemize}
\end{itemize}

\note{
Autre représentation: externe:

\centerline{\includegraphics[width=8cm]{Figures/03-external_bst.pdf}}
}
\end{frame}

\begin{frame}{Tri avec un arbre binaire de recherche}

\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Binary-Search-Tree-Sort}(A)$}
    \li $T\gets$ ``Empty binary search tree''
    \li \For $i\gets 1$ \To $n$
    \li \Do $\proc{Tree-Insert}(T,A[i])$\End
    \li $\proc{Inorder-Tree-Walk}(\attrib{T}{root})$
    \End
  \end{codebox}
  }
\end{small}
\end{center}

\begin{itemize}
\item Exemple: $A=[6,5,7,2,5,8]$

~\hfill\includegraphics[width=5cm]{Figures/05-onebst.pdf}
\item Complexité en temps identique au quicksort
\begin{itemize}
\item Insertion: en moyenne, $n\cdot O(\log n)=O(n\log n)$, pire cas: $O(n^2)$
\item Parcours de l'arbre en ordre: $O(n)$
\item Total: $O(n\log n)$ en moyenne, $O(n^2)$ pour le pire cas
\end{itemize}
\item Complexité en espace cependant plus importante, pour le stockage de la structure d'arbres.
\end{itemize}

\note{File à priorité avec un arbre:
pire cas: $O(N)$ pour l'insertion et l'extraction. Cas moyen: $O(log n)$ dans les deux cas.
}

\end{frame}

\begin{frame}{Dictionnaires: jusqu'ici}

  \begin{center}\small
    \def\arraystretch{1.5}\renewcommand{\tabcolsep}{1mm}
    \begin{tabular}{@{}lcccccc@{}}
    &\multicolumn{3}{c}{\emph{Pire cas}} & \multicolumn{3}{c}{\emph{En moyenne}}\\
    \emph{Implémentation}& \proc{Search} & \proc{Insert} & \proc{Delete} & \proc{Search} & \proc{Insert} & \proc{Delete}\\
    \hline\hline
    Liste &$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$\\
    \hline
    Vecteur trié&$\Theta(\log n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(\log n)$&$\Theta(n)$&$\Theta(n)$\\
\hline
ABR&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$\\
    \hline\hline
  \end{tabular}
  \end{center}

\bigskip

\begin{itemize}
\item Peut-on obtenir $\Theta(\log n)$ dans le pire cas? Oui !
\item Deux solutions:
\begin{itemize}
\item Utiliser de la randomisation pour que le probabilité de
  rencontrer le pire cas soit négligeable
\item Maintenir les arbres équilibrés
\end{itemize}
\end{itemize}

\note{
randomization: n'assure pas qu'on ne sera jamais dans le pire cas. Suppose qu'on puisse jouer sur l'ordre d'insertion
}
\end{frame}


\subsection{Arbres équilibrés AVL}

\begin{frame}{Plan}

\tableofcontents[currentsection]

\end{frame}


\begin{frame}{Arbres équilibrés}
\begin{itemize}
\item Solution générale pour obtenir une complexité au pire cas en $O(\log n)$:% maintenir en permanence un arbre plus ou moins complet
\begin{itemize}
\item Définir un \alert{invariant} sur la structure d'arbre
\item Prouver que cet invariant garantit une hauteur $\Theta(\log n)$
\item Implémenter les opérations d'insertion et suppression de manière à maintenir l'invariant
\item Si ces opérations ne sont pas trop coûteuses (p.ex., $O(\log
  n)$), on aura gagné
\end{itemize}

\bigskip

\item Plusieurs types d'arbres équilibrés:
\begin{itemize}
\item \alert{Arbres AVL}
\begin{itemize}
\item \alert{Invariant: Arbres $H$-équilibrés}
\end{itemize}
\item Arbres 2-3-4
\item Arbres rouges et noirs
\item Splay trees, Scapegoat trees, treaps\ldots
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Arbres $H$-équilibrés}
\begin{itemize}
\item \alert{Définition:}

$$T\mbox{ est }H-\mbox{équilibré}\Leftrightarrow |h(g(T'))-h(d(T'))|\leq 1,$$
pour tout sous-arbre $T'$ de $T$, et où $g(X)$, $d(X)$ et $h(X)$ sont resp. le sous-arbre gauche, le sous-arbre droit et la hauteur de l'arbre $X$.

\medskip

\emph{(Les hauteurs des deux sous-arbres d'un même n\oe ud diffèrent au plus de un)}

\bigskip

\item \alert{Propriété:}

Pour tout arbre $H$-équilibré de taille $n$ et de hauteur $h$, on a 
$$h=\Theta(\log n)$$

Plus précisément, on peut prouver:
 $$\log(n+1)\leq h+1< 1,44 \log(n+2)$$
\end{itemize}

\end{frame}

\begin{frame}{Arbres $H$-équilibrés}
\alert{Démonstration}

Etant donné un arbre $H$-équilibré de taille $n$ et de hauteur $h\geq 1$, pour $h$ fixé, $n$ est
\begin{itemize}
\item Maximum: quand l'arbre est complet, soit quand
$n=2^{h+1}-1\Rightarrow n+1\leq 2^{h+1} \Rightarrow \log (n+1)\leq h+1 \Rightarrow h\in\Omega(\log n) $
\item Minimum: quand $n=N(h)$ où $N(h)$ est la taille d'un arbre $H$-équilibré de hauteur $h$ qui a le moins d'éléments.
\begin{itemize}
\item $N(h)$ peut être défini par récurrence par $N(h)=1+N(h-1)+N(h-2)$ avec $N(0)=1$ et $N(1)=2$.
\end{itemize}
\end{itemize}
\centerline{\includegraphics[width=5cm]{Figures/05-avlmaximum.pdf}}

\note{Pour que le nombre n\oe uds soit minimal, il faut que l'arbre soit déséquilibré (sinon, on pourrait enlever des n\oe uds et toujours satisfaire la propriété d'arbre $H$-équilibré)}

\end{frame}

\begin{frame}{~}%{Borne plus simple}
\begin{itemize}
\item[]
\begin{itemize}
\item On a donc\\
\begin{tabular}{cl}
& $N(h)=1+N(h-1)+N(h-2)$\\
$\Rightarrow$ & $N(h)>2 N(h-2)$ (car $N(h-1)>N(h-2)$)\\
$\Rightarrow$ & $N(h)>2^{h/2}$\\
$\Rightarrow$ & $h< 2\log N(h)$\\
\end{tabular}
\item dont on peut tirer que
$h\in O(\log n)$
\end{itemize}
\item On en déduit que
$$h=\Theta(\log n)$$\qed
\end{itemize}

\note{\begin{eqnarray*}
N(h)&<&2 N(h-2)\\
&<& 2^2 N(h-4)\\
&<& 2^3 N(h-6)\\
&<&\ldots\\
&<&2^i N(h-2i)\\
&<&2^(h/2) N(0)=2^(h/2)
\end{eqnarray*}

}
\end{frame}

\begin{frame}{Borne supérieure plus précise}
\begin{itemize}
\item[]
\begin{itemize}
\item En notant $F(h)=N(h)+1$, on a $F(h)=F(h-1)+F(h-2)$ avec $F(0)=2$, $F(1)=3$
\item $F$ est un récurrence de Fibonacci qui a pour solution
 $$F(h)=\frac{1}{\sqrt{5}} (\phi^{h+3}-\phi'^{h+3})\mbox{ avec }\phi=\frac{1+\sqrt{5}}{2}\mbox{ et }\phi'=\frac{1-\sqrt{5}}{2}$$
\item On a $$N(h)+1=\frac{1}{\sqrt{5}} (\phi^{h+3}-\phi'^{h+3})$$
\item ce qui donne
$$n+1\geq \frac{1}{\sqrt{5}} (\phi^{h+3}-\phi'^{h+3})> \frac{1}{\sqrt{5}} (\phi^{h+3}-1)$$
(car $|\phi'|<1$)
\item En prenant le $\log_{\phi}$ des deux membres:
$$h+1<1,44\log (n+2)$$
\end{itemize}
%A VERIFIER
\end{itemize}

\end{frame}

\begin{frame}{Arbres AVL}

\begin{itemize}
\item \alert{Définition:} Un arbre AVL est un arbre binaire de
  recherche $H$-équilibré
\item Inventé par Adelson-Velskii et Landis en 1960
\item Recherche:
\begin{itemize}
\item Par la fonction $\proc{Tree-Search}$ puisque c'est un arbre binaire
\item Complexité $\Theta(\log n)$ étant donné la propriété
\end{itemize}
\item Insertion:
\begin{itemize}
\item On insère l'élément comme dans un arbre binaire classique
\item On vérifie que l'invariant est respecté
\item Si ce n'est pas le cas, on modifie l'arbre
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Rotations}

\centerline{\includegraphics[width=10cm]{Figures/05-rotations.pdf}}

\bigskip

\begin{center}
\fcolorbox{white}{Lightgray}{%
    \begin{codebox}
      \Procname{$\proc{Left-Rotate}(x)$}
      \li $r\gets\attrib{x}{right}$
      \li $\attrib{x}{right}\gets\attrib{r}{left}$
      \li $\attrib{r}{left}\gets x$
      \li \Return $r$
    \end{codebox}}
~~~~~~~~~~~~~~~~~~~
\fcolorbox{white}{Lightgray}{%
    \begin{codebox}
      \Procname{$\proc{Right-Rotate}(x)$}
      \li $l\gets\attrib{x}{left}$
      \li $\attrib{x}{left}\gets\attrib{l}{right}$
      \li $\attrib{l}{right}\gets x$
      \li \Return $l$
    \end{codebox}}
\end{center}

Les rotations maintiennent la propriété d'arbre binaire  de recherche

\note{Deux types d'opération pour maintenir l'équilibre: rotations à gauche et à droite. Implémentée comme sur ce slide. Opération d'ordre $O(1)$.

\bigskip

Donner un exemple: 
\centerline{\includegraphics[width=10cm]{Figures/03-rotation.pdf}}
}
\end{frame}

\begin{frame}{Insertion dans un AVL}

\centerline{\includegraphics[width=10cm]{Figures/05-avlinsertion.pdf}}

\bigskip

\begin{itemize}
\item Insérer le nouvel élément comme dans un arbre binaire de recherche ordinaire
\item L'insertion peut créer un déséquilibre (l'arbre n'est plus $H$-équilibré)
\item Remonter depuis le nouveau n\oe ud jusqu'à la racine en
  restaurant l'équilibre des sous-arbres rencontrés si nécessaire
\end{itemize}

\note{Implémentation récursive:
%voir ici:http://www.enseignement.polytechnique.fr/profs/informatique/Luc.Maranget/421/poly/arbre-bin.html
}

\end{frame}

% Insertion:
% - si l'arbre est équilibré -> pas de risque de déséquilibrage
% - si déséquilibre à gauche ou à droite -> possibilité de violation dans le cas d'une insertion à droite ou à gauche
% - symétrique:
%   deux cas: 1) -> 1 rotation corrige le tir
%             2) -> 2 rotations corrigent le tir

% Implémentation: on doit maintenir la hauteur des noeuds (data augmentation)
% Au plus 2 rotations par insertion: Intuitivement, les opérations du slide précédent font que la hauteur du sous-arbre en $x$ n'est finalement pas augmentée suite à l'insertion. Tous les sous-arbres au dessus de $x$ sont maintenus équilibrés (et $x$ est le premier sous-arbre non $H$-équilibré).

% Deletion: idem mais plus de rotations sont possibles

\begin{frame}{Equilibrage}

\begin{itemize}
\item Soit $x$ le n\oe ud le plus bas violant l'invariant après l'insertion
  \begin{itemize}
  \item Tous ses sous-arbres sont $H$-équilibrés
  \item Il y a une différence d'au plus 2 niveaux entre ses
    sous-arbres gauche et droit
  \end{itemize}
\item Comment rétablir l'équilibre ?
\item Deux cas possibles (selon insertion à droite ou à gauche):
\end{itemize}
\begin{center}
Cas 1\hspace{4cm}Cas 2

\medskip

\includegraphics[width=8cm]{Figures/05-avlcas1-2.pdf}

\medskip
(Déséquilibre à droite)\hspace{1.3cm}(Déséquilibre à gauche)
\end{center}

\note{Tous ses sous-arbres sont $H$-équilibrés puisque c'est le plus bas qui viole l'invariant}

\end{frame}

\begin{frame}{Cas 1: déséquilibre à droite}
\begin{itemize}
\item Deux sous-cas possibles
\end{itemize}

\begin{center}
Cas 1.1\hspace{4cm}Cas 1.2

\medskip

\includegraphics[width=8cm]{Figures/05-avlcas1.pdf}

\medskip
Déséquilibre à l'extérieur\hspace{1.3cm}Déséquilibre à l'intérieur\\
(Cas droite-droite)\hspace{2cm}(Cas droite-gauche)
\end{center}

\bigskip

\emph{(Pourquoi le cas B et C de hauteur $h$ n'est pas possible ?)}

\note{Leur demander pourquoi ce sont les seuls deux cas.

Pourquoi pas B et C de hauteur h ? Parce que sinon, ça voudrait dire que la hauteur du sous-arbre en $y$ n'a pas augmenté (et donc il ne pourrait pas être devenu déséquilibré).

}
\end{frame}

\begin{frame}{Cas 1.1: déséquilibre à droite, extérieur (droite-droite)}
\begin{itemize}
\item Equilibre rétabli par une rotation à gauche de $x$
\end{itemize}

\begin{center}
\includegraphics[width=10cm]{Figures/05-avleqcas11.pdf}
\end{center}

\end{frame}

\begin{frame}{Cas 1.2: déséquilibre à droite, intérieur (droite-gauche)}
\begin{itemize}
\item Une rotation à gauche ne permet pas de rétablir l'équilibre
%\end{itemize}

\begin{center}
\includegraphics[width=10cm]{Figures/05-avlcas12-wrong.pdf}
\end{center}
\item Le sous-arbre $B$ contient au moins un élément (l'élément inséré)
\begin{center}
\includegraphics[width=7cm]{Figures/05-avlcas12-decomp.pdf}
\end{center}

\end{itemize}

\note{Pourquoi pas $B_l$ et $B_r$ de hauteur $h-1$ ? Parce qu'alors l'insertion n'aurait pas modifié la hauteur de $B$}
\end{frame}

\begin{frame}{Cas 1.2: déséquilibre à droite, intérieur (droite-gauche)}
\begin{itemize}
\item Equilibre rétabli par deux rotations
%\end{itemize}

\begin{center}
\includegraphics[width=10cm]{Figures/05-avlcas12-double.pdf}
\end{center}

\end{itemize}

\end{frame}

\begin{frame}{Cas 2: déséquilibre à gauche}
\begin{itemize}
\item Symétrique du cas 1
\item Deux sous-cas possibles
\end{itemize}

\begin{center}
Cas 2.1\hspace{4cm}Cas 2.2

\medskip

\includegraphics[width=8cm]{Figures/05-avlcas2.pdf}

\medskip
Déséquilibre à l'extérieur\hspace{1.3cm}Déséquilibre à l'intérieur\\
(Cas gauche-gauche)\hspace{2cm}(Cas gauche-droite)
\end{center}

\begin{itemize}
\item Résolus respectivement par une rotation (à droite) et une double rotation.
\end{itemize}

\end{frame}

\begin{frame}{Implémentation}
\begin{itemize}
\item Algorithme récursif: Pour insérer une clé dans un arbre $T$:
\begin{itemize}
\item On l'insère (récursivement) dans le sous-arbre approprié (gauche ou droit)
\item Si l'arbre résultant $T$ devient déséquilibré, on effectue une rotation simple ou double selon le cas dans lequel on se trouve
\end{itemize}
\item L'arbre après rééquilibrage étant de la même hauteur qu'avant l'insertion, on n'aura à faire qu'au plus une rotation (simple ou double).
\item L'implémentation est facilitée si on maintient en chaque n\oe ud $x$ un attribut $\attrib{x}{h}$ avec la hauteur du sous-arbre en $x$.
\item Complexité:
\begin{itemize}
\item $O(h)$ où $h$ est la hauteur de l'arbre,
\item c'est-à-dire $O(\log n)$ vu que l'arbre est $H$-équilibré.
\end{itemize}
\end{itemize}
\note{Exemple:
\centerline{\includegraphics[width=5cm]{Figures/03-avlexemple.pdf}}
}
\end{frame}

\begin{frame}{Suppression}
\begin{itemize}
\item Comme pour l'insertion, on doit rétablir l'équilibre suite à la suppression
\item La suppression d'un n\oe ud peut déséquilibrer le parent de ce n\oe ud
\item Contrairement à l'insertion, on peut devoir rééquilibrer plusieurs ancêtres du n\oe ud supprimé.
\item Chaque rotation étant d'ordre $O(1)$, la complexité d'une suppression reste cependant $O(h)$ pour un arbre de hauteur $h$ et donc $O(\log n)$ pour un AVL.
\end{itemize}
\end{frame}

\begin{frame}{Tri avec un AVL}

\begin{itemize}
\item Comme avec un arbre de binaire de recherche ordinaire, on peut trier avec un AVL
\begin{itemize}
\item On insère les éléments successivement dans l'arbre
\item On effectue un parcours en ordre de l'arbre
\end{itemize}
\item Complexité en temps: $\Theta(n\log n)$ (comme pour le tri par tas)
\item Complexité en espace: $\Theta(n)$ (pour la structure d'arbre temporaire) (versus $O(1)$ pour le heap-sort)
%\item Tas: optimisé pour retrouver et supprimer le minimum (ou le max)
%\item AVL: optimisé pour retrouver et supprimer un élément arbitraire 
%\item A ELABORER
\end{itemize}

\end{frame}


%%%%

\begin{frame}{Dictionnaires: jusqu'ici}

  \begin{center}\small
    \def\arraystretch{1.5}\renewcommand{\tabcolsep}{1mm}
    \begin{tabular}{@{}lcccccc@{}}
    &\multicolumn{3}{c}{\emph{Pire cas}} & \multicolumn{3}{c}{\emph{En moyenne}}\\
    \emph{Implémentation}& \proc{Search} & \proc{Insert} & \proc{Delete} & \proc{Search} & \proc{Insert} & \proc{Delete}\\
    \hline\hline
    Liste &$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$\\
    \hline
    Vecteur trié&$\Theta(\log n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(\log n)$&$\Theta(n)$&$\Theta(n)$\\
\hline
ABR&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$\\
\hline
AVL&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$\\
    \hline\hline
  \end{tabular}
  \end{center}

\bigskip

\begin{itemize}
\item Peut-on faire mieux?
\item Oui, en changeant radicalement de philosophie
\end{itemize}

\note{Dire que les autres algos d'arbres ont la m\^eme complexit\'e.

\bigskip

Appli: système de réservation: on veut retrouver le prochain (min), }

\end{frame}


\begin{frame}{Demo}
Illustrations:
\begin{small}
\begin{itemize}
\item \url{http://people.ksp.sk/~kuko/bak/}
\item \url{http://www.csi.uottawa.ca/~stan/csi2514/applets/avl/BT.html}
\item \url{http://www.cs.jhu.edu/~goodrich/dsa/trees/avltree.html}
\end{itemize}
\end{small}

\note{Montrer les heap min et max, le BST normal, les AVL et puis peut-être les skip-list

~\bigskip

Pour AVL, demander ce qui va se passer lorsqu'on insère un élément dans l'arbre
}
\end{frame}

\section{Tables de hachage}

\begin{frame}{Plan}

\tableofcontents[currentsection]

\end{frame}

\subsection{Principe}

\begin{frame}{Tableau à accès direct}

\begin{itemize}
\item On suppose:
\begin{itemize}
\item que chaque élément a une clé tirée d'un univers
  $U=\{0,1,\ldots,m-1\}$ où $m$ n'est pas trop grand
\item qu'il ne peut pas y avoir deux éléments avec la même clé.
\end{itemize}
\item Le dictionnaire est implémenté par un tableau $T[0\ldots m-1]$:
\begin{itemize}
\item Chaque position dans la table correspond à une clé de $U$.
\item S'il y a un élément $x$ avec la clé $k$, alors $T[k]$ contient un pointeur vers $x$.
\item Sinon, $T[k]$ est vide ($T[k]=\const{NIL}$).
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Tableau à accès direct}

\centerline{\includegraphics[width=9cm]{Figures/05-directtable.pdf}}

\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Direct-Address-Search}(T,k)$}
    \li \Return $T[k]$
\end{codebox}}

\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Direct-Address-Insert}(T,x)$}
    \li \Return $T[\attrib{x}{key}]=x$
\end{codebox}}

\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Direct-Address-Delete}(T,x)$}
    \li \Return $T[\attrib{x}{key}]=\const{NIL}$
\end{codebox}}
\end{small}
\end{center}


\end{frame}

\begin{frame}{Tableau à accès direct}

\begin{itemize}
\item Complexité de toutes les opérations: $O(1)$ (dans tous les cas)
\item Problème:
\begin{itemize}
\item Complexité en espace: $\Theta(|U|)$
\item si l'univers de clés $U$ est grand, stocker une table de taille $|U|$ peut être peu pratique, voire impossible
\end{itemize}
\item Souvent l'ensemble des clés réellement stockées, noté $K$, est petit comparé à $U$ et donc l'espace alloué est gaspillé.

\bigskip\bigskip

\item Comment bénéficier de l'accès rapide d'une table à accès direct avec une table de taille raisonnable ?\\

\medskip

$\Rightarrow$ \alert{Table de hachage:}
\begin{itemize}
\item Réduit le stockage à $\Theta(|K|)$
\item Recherche en $O(1)$ (\alert{en moyenne} !)
\end{itemize}
\end{itemize}
\note{Le prix à payer est qu'on a plus une complexité en $O(1)$ dans le pire cas mais en moyenne. Ce qui n'est pas un problème}
\end{frame}

\begin{frame}{Table de hachage}

\begin{itemize}
\item Inventée en 1953 par Luhn
\item Idée:
\begin{itemize}
\item Utiliser une table $T$ de taille $m\ll|U|$
\item stocker $x$ à la position $h(\attrib{x}{key})$, où $h$ est une fonction de \alert{hachage}: $$h:U\rightarrow \{0,\ldots,m-1\}$$
\end{itemize}
\end{itemize}

\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Hash-Insert}(T,x)$}
    \li $T[h(\attrib{x}{key})]\gets x$
\end{codebox}}
~~~~~~~~\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Hash-Delete}(T,x)$}
    \li $T[h(\attrib{x}{key})]\gets \const{NIL}$
\end{codebox}}

\bigskip

\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Hash-Search}(T,x)$}
    \li \Return $T[h(\attrib{x}{key})]$
\end{codebox}}
\end{small}
\end{center}

Est-ce que ces algorithmes sont corrects ?

\end{frame}

\begin{frame}{Table de hachage: collisions}

\centerline{\includegraphics[width=7cm]{Figures/05-hashtable1.pdf}}

\bigskip

\begin{itemize}
\item \alert{Collision:} lorsque deux clés distinctes $k_1$ et $k_2$ sont telles que $h(k_1)=h(k_2)$
\item Cela se produit toujours lorsque le nombre de clés observées est plus grand que la taille du tableau $T$ ($|K|>m$)
\item Très probable, même lorsque la fonction de hachage répartit les clés uniformément $\Rightarrow$ \alert{Paradoxe des anniversaire}
\end{itemize}
\note{Supposons qu'on veuille stocker de l'info sur vous dans un calendrier avec 365 cases}
\end{frame}

\begin{frame}{Paradoxe des anniversaires}

\begin{itemize}
\item Hypothèse:
\begin{itemize}
\item On néglige les années bissextiles
\item Les 365 jours présentent la même probabilité d'être un jour d'anniversaire
\end{itemize}
\item Si $p$ est la probabilité d'une collision d'anniversaires:
$$1-p = \frac{364}{365} \cdot \frac{363}{365} \cdot \frac{362}{365} \ldots \frac{365-(n-1)}{365} = \frac{365!}{(365-n)! 365^n}$$
Exemples:
\begin{itemize}
\item $n=23 \Rightarrow p>0,5$
\item $n=57 \Rightarrow p>0,99$
\item $n=70 \Rightarrow p>0,999$
\end{itemize}
\bigskip

\item Pour une table de hachage:
\begin{itemize}
\item $m=365$ et 57 clés $\Rightarrow$ plus de 99\% de chance de collision
\item $m=1000000$ et 2500 clés $\Rightarrow$ plus de 95\% de chance de collision
\end{itemize}
\end{itemize}
\note{Faire l'essai dans la classe: demander qui est ne en avril-mai}
\end{frame}

\begin{frame}{Collision}

\begin{itemize}
\item Pour éviter les collisions:
\begin{itemize}
\item on veille à utiliser une fonction de hachage qui disperse le
  plus possible les clés vers les différents compartiments.
\item on utilise un nombre de compartiments suffisamment grand
\end{itemize}
Cependant, même dans ce cas, la probabilité de collision peut
  être non négligeable.

\bigskip

\item Deux approches pour prendre en compte les collisions:
\begin{itemize}
\item Le chaînage (adressage fermé)
\item Le sondage (adressage ouvert)
\end{itemize}
\end{itemize}

\note{Voir plus loin pour la première propriété

\bigskip

On va d'abord voir la première solution. On verra ensuite la seconde}

\end{frame}


\begin{frame}{Résolution des collisions par chaînage}

Solution: mettre les éléments qui sont ``hachés'' vers la même
position dans une liste liée (simple ou double)

\bigskip

\centerline{\includegraphics[width=10cm]{Figures/05-hashtable2.pdf}}

\note{Une simple ou une double ? Une liste double est meilleure pour supprimer des éléments}
\end{frame}

\begin{frame}{Implémentation des opérations}

~\bigskip

\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Chained-Hash-Insert}(T,x)$}
    \li $\proc{List-Insert}(T[h(\attrib{x}{key})],x)$
\end{codebox}}
~~~~~~~~\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Chained-Hash-Delete}(T,x)$}
    \li $\proc{List-Delete}(T[h(\attrib{x}{key})],x)$
\end{codebox}}

\bigskip

\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Chained-Hash-Search}(T,k)$}
    \li \Return $\proc{List-Search}(T[h(k)],k)$
\end{codebox}}
\end{small}
\end{center}

\bigskip
\bigskip

\begin{itemize}
\item Complexité:
\begin{itemize}
\item Insertion: $O(1)$
\item Suppression: $O(1)$ si liste doublement liée, $O(n)$ pour une liste de taille $n$ si liste simplement liée.
\item Recherche: $O(n)$ si liste de taille $n$.
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Analyse du cas moyen}

%\medskip

\begin{itemize}
\item Recherche d'une clé $k$ dans la table:
\begin{itemize}
\item recherche positive: la clé $k$ se trouve dans la table
\item recherche négative: la clé $k$ n'est pas dans la table
\end{itemize}
\medskip
\item Le \alert{facteur de charge} d'une table de hachage est donné par $\alpha=\frac{n}{m}$ où:
\begin{itemize}
\item $n$ est le nombre d'éléments dans la table
\item $m$ est la taille de la table (c'est-à-dire, le nombre de listes liées)
\end{itemize}
\medskip
\item \alert{hachage uniforme simple}: Pour toute clé $k\in U$,
$$Proba\{h(k)=i\}=\frac{1}{m}, \forall i\in\{0,\ldots,m-1\}$$

\bigskip

\centerline{\includegraphics[width=7cm]{Figures/05-hachageuniforme.pdf}}

\end{itemize}

\end{frame}

\begin{frame}{Analyse du cas moyen}
\begin{itemize}
\item Hypothèses:
\begin{itemize}
\item $h$ produit un hachage uniforme simple
\item le calcul de $h(k)$ est $\Theta(1)$
\item Insertion en début de liste
\end{itemize}
\item $\Rightarrow$ complexités moyennes:
\begin{itemize}
\item recherche négative: $\Theta(1+\alpha)$
\item recherche positive: $\Theta(1+\alpha)$
\end{itemize}
\item Si $n=O(m)$, \hfill {\it ($m$ croît au moins linéairement avec $n$)}, $$\alpha=\frac{O(m)}{m}=O(1)$$
\item Toutes les opérations sont donc $O(1)$ en moyenne
\end{itemize}

\note{
Pourquoi est-ce qu'on n'écrit pas directement $\Theta(\alpha)$ ? Parce que $\alpha=n/m=f(n)$ et pour que l'analyse ait un sens, il faut que $m$ grandisse avec $n$. Donc, on ne sait pas a priori comment $\alpha$ évolue avec $n$ et donc si on peut négliger 1 par rapport à $\alpha$

\bigskip


Si $n=O(m)$, on a $m=\Omega(n)$, ce qui veut dire que la taille de la table croît proportionnellement avec la taille des données. Si elle croît moins que linéairement, lorsque $n$ va croître le alpha va augmenter.}
\end{frame}

\begin{frame}{Analyse du cas moyen: recherche négative}
\begin{itemize}
\item La clé $k$ ne se trouve pas dans la table
\item Par la propriété de hachage uniforme simple, elle a la même probabilité d'être envoyée vers chaque position dans la table.
\item Recherche négative requière le parcours de la liste $T[h(k)]$ complète
\item Cette liste a une longueur moyenne $E[n_{h(k)}]=\alpha$.
\item Le nombre d'éléments à examiner lors d'une recherche négative est donc $\alpha$.
\item En ajoutant le temps de calcul de la fonction de hachage, on arrive à une complexité moyenne $\Theta(1+\alpha)$.
\end{itemize}
\end{frame}


\begin{frame}{Analyse du cas moyen: recherche positive}

\begin{itemize}
\item La clé $k$ se trouve dans la table
\item Supposons qu'elle ait été insérée à la $i$-ième étape (parmi $n$).
\begin{itemize}
\item Le nombre d'éléments à examiner pour trouver la clé est le
  nombre d'éléments insérés à la position $h(k)$ après $k$ plus 1 (la clé $k$ elle-même). 
\item En moyenne, sur les $n-i$ insertions après $k$, il y en aura
  $(n-i)/m$ qui correspondront à la position $h(k)$.
\end{itemize}
\item $k$ ayant pu être insérée à n'importe quelle étape parmi $n$ avec une probabilité $1/n$:
\begin{footnotesize}
$$\sum_{i=1}^n \frac{1}{n} (1+\frac{n-i}{m})=1+\frac{1}{n m}(\sum_{i=1}^n n-\sum_{i=1}^n i)=1+\frac{1}{n m} (n^2-\frac{n(n+1)}{2})=1+\frac{\alpha}{2}-\frac{\alpha}{2n}$$
\end{footnotesize}
\item En tenant compte du coût du hachage, la complexité en moyenne
  est donc $\Theta(2+\alpha/2-\alpha/2n)=\Theta(1+\alpha)$.
\end{itemize}

%% Intuitivement:
%% Supposons qu'on ait inséré $x$ à la $i$th étape (proba de chaque étape est $1/n$.

%% Nombre de comparaison est = au nombre d'éléments inséré après $x$ dans
%% la table, c'est-à-dire $(n-i+1)/m$ vu l'hypothèse d'uniform hashing

%% Proba totale=$\sum_{i=1}^N 1/N (n-i+1)/m$...

\note{ $2+\alpha/2-\alpha/2n\in \Theta(1+\alpha)$ car $2*(1+\alpha/4)<2+\alpha/2-\alpha/2n<2*(1+\alpha)$ pour $n$ grand}

\end{frame}

\subsection{Fonctions de hachage}

\begin{frame}{Plan}

\tableofcontents[currentsection,currentsubsection]

\end{frame}


\begin{frame}{Fonctions de hachage}
\begin{itemize}
\item Idéalement, la fonction de hachage
\begin{itemize}
\item devrait être facile à calculer ($O(1)$)
\item devrait satisfaire l'hypothèse de hachage uniforme simple
\end{itemize}
\item La deuxième propriété est très difficile à assurer en pratique:
\begin{itemize}
\item La distribution des clés est généralement inconnue
\item Les clés peuvent ne pas être indépendantes
\end{itemize}
\item En pratique, on utilise des heuristiques basées sur la nature
  attendue des clés
\item Si toutes les clés sont connues, il existe des algorithmes pour
  construire une fonction de hachage parfaite, sans collision
  (Exemple: le logiciel gperf)
\end{itemize}

\end{frame}

\begin{frame}{Fonctions de hachage: codage préalable}

\begin{itemize}
\item Les fonctions de hachage supposent que les clés sont des nombres naturels
\item Si ce n'est pas le cas, il faut préalablement utiliser une \alert{fonction de codage}
\item Exemple: codage des chaînes de caractères:
\begin{itemize}
\item On interprète la chaîne comme un entier dans une certaine base
\item Exemple pour ``SDA'': valeurs ASCII (128 possibles): $$S=83, D=68, A=65$$
\item Interprété comme l'entier:\hfill {\it (Pourquoi pas 83+68+65 ?)}
 $$(83\cdot 128^2)+(68\cdot 128^1)+(65\cdot 128^0)=1368641$$
\item Calculé efficacement par la méthode de Horner:
$$((83\cdot 128+68)\cdot 128 + 65)$$
\end{itemize}
\end{itemize}
\note{
Pourquoi ne pas prendre la somme des entiers (83+68+65) ? reste la même pour une permutation de l'entrée

\bigskip

Horner: $$C_n x^n+c_{n-1} x^{n-1}+\ldots+C_0 x^0$$
$$(((C_n x+c_{n-1}) x+c_{n-2})x+\ldots C_1)x+C_0$$
}
\end{frame}

\begin{frame}{Méthode de division}

La fonction de hachage calcule le reste de la division entière de la clé par la taille de la table
$$h(k)=k\bmod m.$$
Exemple: $m=20$ et $k=91$ $\Rightarrow h(k)=11$.

\bigskip

\alert{Avantages:} simple et rapide (juste une opération de division)

\bigskip

\alert{Inconvénients:} Le choix de $m$ est très sensible et certaines valeurs doivent être évitées

\bigskip

Exemples:
\begin{itemize}
\item Si $m=2^p$ pour un entier $p$, $h(k)$ ne dépend que des $p$ bits
  les moins significatifs de $k$\\
\begin{itemize}
\item Exemple: ``SDA'' $\bmod\ 128$= ``GAGA''$\bmod\ 128$=65
\end{itemize}
\item Si $k$ est une chaîne de caractères codée en base $2^p$ et $m=2^p-1$, permuter la chaîne ne modifie pas le valeur de hachage\\
\begin{itemize}
\item Exemple: ``SDA''=1368641, ``DSA''=1124801\\
$\Rightarrow 1368641\bmod 127=1124801\bmod 127=89$
\end{itemize}
\end{itemize}
\note{Théorie qui motive les clés de hachage est la théorie des nombres. On va donner le minimum ici.

\bigskip

Pourquoi pas: $k m/k_{max}$ ? Parce qu'on ne connaît pas $k_{max}$

\bigskip

%Dire que si $m$ est premier, alors la valeur hachée est une
%permutation de $\{0,1,\ldots,m-1\}$

On cherche souvent des nombres premiers sous la forme $2^p-1$ car il
existe un test de primalité efficace pour ces nombres. Dans le cas
général, tester la primalité d'une nombre est très complexe.

}
\end{frame}

\begin{frame}{Méthode de division}

\begin{itemize}
\item Si la fonction de hachage produit des séquences périodiques, il vaut mieux choisir $m$ premier
\item En effet, si $m$ est premier avec $b$, on a:
$$\{(a+b\cdot i) \bmod m|i=0,1,2,\ldots\}=\{0,1,2,\ldots,m-1\}$$
\item Exemple: hachage de $\{206, 211, 216, 221,\ldots\}$
\begin{itemize}
\item $m=100$: valeurs hachées possibles: 6, 11,\ldots, 96
\item $m=101$: toutes les entrées sont exploitées
\end{itemize}

\bigskip

\item[$\Rightarrow$] Bonne valeur de $m$: un nombre premier pas trop près d'une puissance exacte de 2
\end{itemize}
\end{frame}

\begin{frame}{Méthode de multiplication}
\begin{itemize}
\item Fonction de hachage:
$$h(k)=\lfloor m\cdot(k A \bmod 1)\rfloor$$
où
\begin{itemize}
%\item $m$ est la taille de la table de hachage
%\item $k$ est la clé
\item $A$ est une constante telle que $0<A<1$.
\item $k A \bmod 1=k A - \lfloor k A \rfloor$ est la partie
  fractionnaire de $kA$.
\end{itemize}
\item Inconvénient: plus lente que la méthode de division
\item Avantage: la valeur de $m$ n'est plus critique
\item La méthode marche mieux pour certaines valeurs de $A$. Par
  exemple:$$A=\frac{\sqrt{5}-1}{2}$$
\end{itemize}
\end{frame}

\begin{frame}{Méthode de multiplication: implémentation}
Calcul aisé si:
\begin{itemize}
\item $m=2^p$ pour un entier $p$
\item Les mots sont codés en $w$ bits et les clés $k$ peuvent être
  codées par un seul mot
\item $A$ de la forme $s/2^w$ pour $0<s<2^w$
\end{itemize}

\centerline{\includegraphics[width=8cm]{Figures/05-hashtablemultiplication.pdf}}

\bigskip

{\small\it Exemple: $m=2^3$, $w=5$ ($\Rightarrow 0<s<2^5$), $s=13$, $A=13/32$ $\Rightarrow h(21)=4$}
\end{frame}

\subsection{Adressage ouvert}

\begin{frame}{Plan}

\tableofcontents[currentsection,currentsubsection]

\end{frame}


\begin{frame}{Adressage ouvert: principe}

\begin{itemize}
\item Alternative au chaînage pour gérer les collisions
\item Tous les éléments sont stockés dans le tableau (pas de listes chaînées)
\item Ne fonctionne que si $\alpha\leq 1$
\item Pour insérer une clé $k$, on \alert{sonde} les cases
  systématiquement à partir de $h(k)$ jusqu'à en trouver une vide.
\item Différentes méthodes en fonction de la stratégie de sondage
\end{itemize}

\end{frame}

\begin{frame}{Adressage ouvert: stratégie de sondage}

On définit une nouvelle fonction de hachage qui dépend de la clé
  et du numéro du sondage:
$$h:U\times\{0,1,\ldots,m-1\} \rightarrow \{0,1,\ldots,m-1\}$$
et qui est telle que
$$\langle h(k,0), h(k,1), \ldots, h(k,m-1)\rangle$$
est une permutation de $\langle 0, 1, \ldots, m-1\rangle$.
%\item La table peut donc être totalement remplie et la suppression est difficile.


\centerline{\includegraphics[width=7cm]{Figures/05-hashopenaddressing.pdf}}

\end{frame}

\begin{frame}{Adressage ouvert: recherche et insertion}

\begin{center}
\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Hash-Search}(T,k)$}
    \li $i\gets 0$
    \li \Repeat
    \li $j\gets h(k,i)$
    \li \If $T[j]\isequal k$
    \li \Then \Return j\End
    \li $i\gets i+1$
    \li \Until $T[j]\isequal \const{NIL}$ or $i\isequal m$
    \li \Return $\const{NIL}$
\end{codebox}}~~~\fcolorbox{white}{Lightgray}{%
  \begin{codebox}
    \Procname{$\proc{Hash-Insert}(T,k)$}
    \li $i\gets 0$
    \li \Repeat
    \li $j\gets h(k,i)$
    \li \If $T[j]\isequal \const{NIL}$
    \li \Then  $T[j]=k$
    \li \Return j
    \li \Else $i\gets i+1$ \End
    \li \Until $i\isequal m$
    \li \Error ``hash table overflow''
\end{codebox}}

\end{center}

\centerline{\includegraphics[width=7cm]{Figures/05-hashopenaddressing.pdf}}

\note{Suppression ??}

\end{frame}

\begin{frame}{Adressage ouvert: suppression}
\begin{itemize}
\item La suppression est possible mais pas aisée
\begin{itemize}
\item On évitera l'utilisation de l'adressage ouvert si on prévoit de nombreuses suppressions de clés dans le dictionnaire
\end{itemize}
\item On ne peut pas naïvement mettre $\const{NIL}$ dans la case contenant la clé $k$ qu'on désire effacer%\hfill (\emph{Pourquoi ?})
\item Solution:
\begin{itemize}
\item Utiliser une valeur spéciale $\const{DELETED}$ au lieu
  de $\const{NIL}$ pour signifier qu'on a effacé une valeur dans cette case
\item Lors d'une recherche: considérer un case contenant
  $\const{DELETED}$ comme une case contenant une clé
\item Lors d'une insertion: considérer une case contenant $\const{DELETED}$ comme une case vide.
\end{itemize}
\item Inconvénient: le temps de recherche ne dépend maintenant plus du facteur de charge $\alpha$ de la table%\hfill(\emph{Pourquoi ?})
\end{itemize}
\note{Demander pourquoi le temps de recherche ne dépend plus du facteur de charge (plutôt du facteur de charge maximum sur la durée de vie de la table)}
\end{frame}

\begin{frame}{Stratégies de sondage}
\begin{itemize}
\item Soit $h_k=\langle h(k,0), h(k,1), \ldots, h(k,m-1)\rangle$ la séquence de sondage correspondant à la clé $k$.
\item Hachage uniforme:
\begin{itemize}
\item chacun des $m!$ permutations de $\langle
  0,1,\ldots,m-1\rangle$ a la même probabilité d'être la séquence de
  sondage d'une clé $k$.
\item Difficile à implémenter.
\end{itemize}
\item En pratique, on se contente d'une garantie que la séquence de
  sondage soit une permutation de $\langle
  0,1,\ldots,m-1\rangle$.

\bigskip

\item Trois techniques pseudo-uniformes:
\begin{itemize}
\item sondage linéaire
\item sondage quadratique
\item double hachage
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Sondage linéaire}

\centerline{\includegraphics[width=7cm]{Figures/05-linearprobing.pdf}}

\bigskip

$$h(k,i)=(h'(k)+i) \bmod m,$$
où $h'(k)$ est une fonction de hachage ordinaire à valeurs dans $\{0,1,\ldots,m-1\}$.

%\bigskip

Propriétés:
\begin{itemize}
\item très facile à implémenter
\item effet de grappe fort: création de longues suites de cellules occupées
\begin{itemize}
\item La probabilité de remplir une cellule vide est $\frac{i+1}{m}$ où $i$ est le nombre de cellules pleines précédant la cellule vide
\end{itemize}
\item pas très uniforme
\end{itemize}

\note{On sonde les positions qui suivent directement la case trouvée en cyclant sur le tableau

\bigskip

longues suites augmentent les temps de calcul pour l'insertion et la recherche: on tombe souvent sur une collision}

\end{frame}

\begin{frame}{Sondage quadratique}

$$h(k,i)=(h'(k)+c_1 i+ c_2 i^2) \bmod m,$$
où $h'$ est une fonction de hachage ordinaire à valeurs dans $\{0,1,\ldots,m-1\}$, $c_1$ et $c_2$ sont deux constantes non nulles.

\bigskip

Propriétés:
\begin{itemize}
\item nécessité de bien choisir les constantes $c_1$ et $c_2$ (pour
  avoir une permutation de $\langle 0,1,\ldots,m-1\rangle$)
\item effet de grappe plus faible mais tout de même existant:
\begin{itemize}
\item  Deux clés de même valeur de hachage suivront le même chemin
$$h(k,0)=h(k',0)\Rightarrow h(k,i)=h(k',i)$$
\end{itemize}
\item meilleur que le sondage linéaire
\end{itemize}

\note{probleme: le saut ne depend pas de la clé $\rightarrow$ on crée des grappes malgré tout. Solution: rendre le saut dépendant de la clé (double hachage)}

\end{frame}

\begin{frame}{Double hachage}

\begin{columns}
\begin{column}{9cm}

$$h(k,i)=(h_1(k)+i h_2(k))\bmod m,$$
où $h_1$ et $h_2$ sont des fonctions de hachage ordinaires à valeurs dans $\{0,1,\ldots,m-1\}$.

\bigskip

Propriétés:
\begin{itemize}
\item difficile à implémenter à cause du choix de $h_1$ et $h_2$ ($h_2(k)$ doit être premier avec $m$ pour avoir une permutation de $\langle 0,1,\ldots,m-1\rangle$).
\item très proche du hachage uniforme
\item bien meilleur que les sondages linéaire et quadratique
\end{itemize}

\bigskip

\emph{Exemple: $h_1(k)=k\bmod 13$, $h_2(k)=1+(k\mod 11)$, insertion de la clé 14}

\end{column}
\begin{column}{2.5cm}
\begin{center}
\includegraphics[width=1.5cm]{Figures/05-doublehachage.pdf}
\end{center}
\end{column}
\end{columns}
\note{Pour la remarque $h_2(k)$ doit être premier avec $m$, voir le slide sur la périodicité}
\end{frame}

\begin{frame}{Adressage ouvert: élément d'analyse}
Pour une table de hachage à adressage ouvert de taille $m$ contenant $n$ éléments ($\alpha=n/m<1$) et en supposant le hachage uniforme
\begin{itemize}
\item Le nombre moyen de sondages pour une recherche négative ou un
  ajout est borné par $\frac{1}{1-\alpha}$
\item Le nombre moyen de sondages pour une recherche positive est borné par $\frac{1}{\alpha} \log \frac{1}{1-\alpha}$
\end{itemize}
%(\emph{pas démontré dans ce cours})

\bigskip

$\Rightarrow$ Si $\alpha$ est constant ($n=O(m)$), la recherche est $O(1)$.
\begin{itemize}
\item Si $\alpha=0.5$, une recherche nécessite en moyenne 2 sondages ($1/(1-0.5)$).
\item Si $\alpha=0.9$, une recherche nécessite en moyenne 10 sondages ($1/(1-0.9)$).
\end{itemize}

\note{Sans démonstration}

\end{frame}

\begin{frame}{Adressage ouvert versus chaînage}
\begin{itemize}
\item Chaînage:
\begin{itemize}
\item Peut gérer un nombre illimité d'éléments et de collisions
\item Performances plus stables
\item Surcoût lié à la gestion et le stockage en mémoire des listes liées
\end{itemize}
\item Adressage ouvert:
\begin{itemize}
\item Rapide et peu gourmand en mémoire
\item Choix de la fonction de hachage plus difficile (pour éviter les grappes)
\item On ne peut pas avoir $n>m$
\item Suppression problématique
\end{itemize}

\bigskip

\item D'autres alternatives existent:
\begin{itemize}
\item Two-probe hashing
\item Cuckoo hashing
\item \ldots
\end{itemize}
\end{itemize}

\note{Cuckoo hashing: on utilise plusieurs fonctions de hachage: si
  collision, on déplace l'élément à une nouvelle position en utilisant
  la fonction de hachage. Si re-collision, on prend la suivante, et
  ainsi de suite. Recherche: on utilise les fonctions de hachage en
  séquence.

\bigskip

Two-probe hashing: deux fonctions de hachage: on hache deux fois en cas de collision et on insère la clé dans la chaîne la plus courte. $\log\log n$ pour la longueur moyenne d'une chaîne.}

\end{frame}

\begin{frame}{Le rehachage}
\begin{itemize}
\item Lorsque $\alpha$ se rapproche de 1, les performances s'effondrent
\item Solution: \alert{rehachage}: création d'une table plus grande
\begin{itemize}
\item allocation d'une nouvelle table
\item détermination d'une nouvelle fonction de hachage, tenant compte du nouveau $m$
\item parcours des entrées de la table originale et insertion dans la nouvelle table
\end{itemize}
\item Si la taille est doublée, le coût asymptotique constant des opérations est conservé (voir slide \pageref{sec04:amortie}).
\end{itemize}

\note{Comment pourrait-on attaquer un système ?}

\end{frame}

\begin{frame}{Universal hashing}
\begin{itemize}
\item Les performances d'un table de hachage se dégrade fortement en
  cas de collisions multiples
\item Connaissant la fonction de hachage, un adversaire malintentionné pourrait s'amuser à entrer des clés créant des collisions. Exemples:
\begin{itemize}
\item Création de fichiers avec des noms bien choisis dans le kernel Linux 2.4.20
\item 28/12/2011: {\scriptsize \url{http://www.securityweek.com/hash-table-collision-attacks-could-trigger-ddos-massive-scale}}
\end{itemize}
\item C'est un exemple d'\alert{attaque par déni de service}

\bigskip

\item Parade: \alert{hachage universel}: choisir la fonction de hachage aléatoirement à chaque création d'une nouvelle instance de la table 
\item Exemple:
$$h(k)=((ak+b) \bmod p)\bmod m,$$
où $p$ est un premier très grand et $a$ et $b$ deux entiers choisis aléatoirement
\end{itemize}

\note{Si il utilise tout le temps la même clé, ça va écraser la valeur et ça ne posera pas de problème

\bigskip

fonction aléatoire $\Rightarrow$ l'utilisateur ne peut pas savoir a priori quelles clés vont créer des collisions
}

\end{frame}

\begin{frame}{Demo}

\begin{small}
\begin{itemize}
\item \url{http://groups.engin.umd.umich.edu/CIS/course.des/cis350/hashing/WEB/HashApplet.htm}
\end{itemize}
\end{small}

\end{frame}

\subsection{Comparaisons}

\begin{frame}{Plan}

\tableofcontents[currentsection,currentsubsection]

\end{frame}

\begin{frame}{Dictionnaires: résumé}

  \begin{center}\small
    \def\arraystretch{1.5}\renewcommand{\tabcolsep}{1mm}
    \begin{tabular}{@{}lcccccc@{}}
    &\multicolumn{3}{c}{\emph{Pire cas}} & \multicolumn{3}{c}{\emph{En moyenne}}\\
    \emph{Implémentation}& \proc{Search} & \proc{Insert} & \proc{Delete} & \proc{Search} & \proc{Insert} & \proc{Delete}\\
    \hline\hline
    Liste &$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$\\
    \hline
    Vecteur trié&$\Theta(\log n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(\log n)$&$\Theta(n)$&$\Theta(n)$\\
\hline
ABR&$\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$\\
\hline
AVL&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$&$\Theta(\log n)$\\
\hline
Table de hachage & $\Theta(n)$&$\Theta(n)$&$\Theta(n)$&$\Theta(1)$&$\Theta(1)$&$\Theta(1)$\\
    \hline\hline
  \end{tabular}
  \end{center}

\begin{itemize}
\item Cas moyen valable uniquement sous l'hypothèse de hachage uniforme
\item Comment obtenir $\Theta(\log n)$ dans le pire cas avec une table de hachage ?
\end{itemize}

\note{Leur demander ici de relever les avantages et inconvénients des arbres et tables de hachage...}
\end{frame}

\begin{frame}{ABR/AVL versus table de hachage}

Tables de hachage:
\begin{itemize}
\item Faciles à implémenter
\item Seule solution pour des clés non ordonnées
\item Accès et insertion très rapides en moyenne (pour des clés simples)
\item Espace gaspillé lorsque $\alpha$ est petit
\item Pas de garantie au pire cas (performances ``instables'')
\end{itemize}

\bigskip

Arbres binaire de recherche (équilibrés):
\begin{itemize}
\item Performance garantie dans tous les cas (stabilité)
\item Taille de structure s'adapte à la taille des données
\item Supportent des opérations supplémentaires lorsque les clés sont ordonnées (parcours en ordre, successeur, prédécesseur, etc.)
\item Accès et insertion plus lente en moyenne
\end{itemize}

\end{frame}


%% \begin{frame}{Applications}

%% Quelles implémentations pour les applications suivantes:
%% \begin{itemize}
%% \item Spotlight
%% \item Itunes song
%% \item IP lookup
%% \item Dictionnaire pour le spell checking
%% \end{itemize}
%% \end{frame}

%% \begin{frame}{Ce qu'on a vu}

%% \begin{itemize}
%% \item D'autres types d'arbres équilibrés: red-black trees, ...
%% \item Fonctions de hachages sophistiquées
%% \item Démonstrations formelles de certaines complexités
%% \end{itemize}

%% \end{frame}
