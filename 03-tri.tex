
\part{Algorithmes de tri}

%% % http://www.sorting-algorithms.com/

\begin{frame}{Plan}

\tableofcontents

\end{frame}

\section{Algorithmes de tri}

\begin{frame}{Tri}

\begin{itemize}
\item Un des problèmes algorithmiques les plus fondamentaux.
\item En général, on veut trier des enregistrements avec une clé et des
  données attachées.
\medskip
\begin{center}
\begin{tabular}{ccccccc}
Record$_1$ & & Record$_2$ & & Record$_3$ & & Record$_n$\\
\cline{1-1} \cline{3-3} \cline{5-5} \cline{7-7}
\multicolumn{1}{|c|}{Key$_1$} & & \multicolumn{1}{|c|}{Key$_2$} & & \multicolumn{1}{|c|}{Key$_3$} & \ldots & \multicolumn{1}{|c|}{Key$_n$}\\
 \cline{1-1} \cline{3-3} \cline{5-5} \cline{7-7}
\multicolumn{1}{|c|}{Data$_1$} & & \multicolumn{1}{|c|}{Data$_2$} & & \multicolumn{1}{|c|}{Data$_3$} & & \multicolumn{1}{|c|}{Data$_n$}\\
 \cline{1-1} \cline{3-3} \cline{5-5} \cline{7-7}
\end{tabular}
\end{center}
\medskip
\item Ici, on va ignorer ces données satellites et se focaliser sur
  les algorithmes de tri

\bigskip

\item Le problème de tri:
\begin{itemize}
\item Entrée: une séquence de $n$ nombres $\langle a_1,a_2,\ldots,a_n\rangle$
\item Sortie: une permutation de la séquence de départ $\langle a_1',a_2',\ldots,a_n'\rangle$ telle que $a_1'\leq a_2'\leq\ldots\leq a_n'$
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Applications}

Applications innombrables:
\begin{itemize}
\item Tri des mails selon leur ancienneté
\item Tri des résultats de requête dans un moteur de recherche
\item Tri des facettes des objets pour l'affichage dans les jeux 3D
\item Gestion des opérations bancaires
\item ...
\end{itemize}

\bigskip

Le tri sert aussi de brique de base pour d'autres algorithmes:
\begin{itemize}
\item Recherche binaire dans un tableau trié
\item Recherche des éléments dupliqués dans une liste
\item Recherche du $k$ème élément le plus grand dans une liste
\item ...
\end{itemize}

\bigskip

Des études montrent qu'environ 25\% du temps CPU des ordinateurs est utilisé pour trier
\end{frame}

\begin{frame}{Différents types de tri}

\begin{itemize}
\item {\bf Tri interne:} tri en mémoire centrale. {\bf Tris externes:} données sur un disque externe.
\item {\bf Tri de tableau:} tri qui trie un tableau. Extensible à toutes structures de données offrant un accès en temps (quasi) constant à ses éléments.
\item {\bf Tri générique:} peut trier n'importe quel type d'objets pour autant qu'on puisse comparer ces objets.
\item {\bf Tri comparatif:} basé sur la comparaison entre les éléments (clés)

\end{itemize}

\end{frame}

\begin{frame}{Différents types de tri}
\begin{itemize}
\item {\bf Tri itératif:} basé sur un ou plusieurs parcours itératifs du tableau
\item {\bf Tri récursif:} basé sur une procédure récursive
\item {\bf Tri en place:} modifie directement la structure qu'il est en train de trier. Ne nécessite qu'une quantité très limitée de mémoire supplémentaire.
\item {\bf Tri  stable:} conserve l'ordre relatif des éléments égaux (au sens de la méthode de comparaison).
\end{itemize}

\note{Stable: expliquer ce que ca veut dire en illustrant avec les colonnes dans excel: tri sur deux colonnes: un tri stable va maintenir l'ordre sur la deuxième colonne

\bigskip

Stable si $$(2,1),(1,2),(2,3),(2,4),(6,5)$$
est trié en $$(1,2),(2,1),(2,3),(2,4),(6,5)$$
}
\end{frame}

\begin{frame}{Jusqu'ici}

  \begin{center}
    \def\arraystretch{1.5}
  \begin{tabular}{@{}lccc@{}c@{}}
    \emph{Algorithme}&\multicolumn{3}{c}{\emph{Complexité}}&\emph{En place?}\\
    & \emph{\small Pire} & \emph{\small Moyenne} & \emph{Meilleure} & \\
    \hline\hline
    \proc{Insertion-Sort}&$\Theta(n^2)$&$\Theta(n^2)$&$\Theta(n)$&oui\\
    \hline
    \proc{Selection-Sort}&$\Theta(n^2)$&$\Theta(n^2)$&$\Theta(n^2)$&oui\\
    \hline
    \proc{Bubble-Sort}&$\Theta(n^2)$&$\Theta(n^2)$&$\Theta(n)$&oui\\
    \hline
    \proc{Merge-Sort}&$\Theta(n\log{n})$&$\Theta(n\log{n})$&$\Theta(n\log{n})$&non\\
    \hline\hline
    \hspace{1em}\alert{??}& &\alert{$\Theta(n\log{n})$}& &\alert{oui}\\
    \hline
    \hspace{1em}\alert{??}&\alert{$\Theta(n\log{n})$}& & &\alert{oui}\\
    \hline\hline
  \end{tabular}
  \end{center}

\note{Expliquer le principe des algorithmes

\bigskip

Demander lesquels sont stables: selection: non, insertion: oui, merge: oui, bubble: oui.
}
\end{frame}

\section{Tri rapide}

\begin{frame}{Tri rapide}
\begin{itemize}
\item {\em Quicksort} en anglais
\item Inventé par Hoare en 1960
\item Dans le top 10 des algorithmes du 20-ième siècle (SIAM)
\item L'exemple le plus célèbre de la technique du ``diviser pour régner''
\item Tri en place, comme tri par insertion, et contrairement au tri par fusion
\item Complexité: $\Theta(n^2)$ dans le pire des cas, $\theta(n\log n)$ en moyenne
\end{itemize}
\end{frame}

\begin{frame}{\proc{QuickSort}: principe}
Pour trier un sous-tableau $A[p\twodots r]$:
\begin{itemize}
\item Partitionner $A[p\twodots r]$ en deux sous-tableaux: $A[p..q-1]$ et $A[q+1\twodots r]$ tels que tout élément de $A[p\twodots q-1]$ est $\leq A[q]$ et $A[q]<$ à tout élément de $A[q+1\twodots r]$.\\~\hfill\alert{\emph{(diviser)}}

\item Appeler récursivement l'algorithme pour trier $A[p\twodots q-1]$ et $A[q+1\twodots r]$\\~\hfill\alert{\emph{(régner)}}
\end{itemize}

\bigskip

Remarques:
\begin{itemize}
\item $A[q]$ est appelé le ``\alert{pivot}''
\item Par rapport au tri par fusion, il n'y a pas d'opération de combinaison
\end{itemize}

\end{frame}

\begin{frame}{\proc{QuickSort}: principe}

\centerline{\includegraphics[width=8cm]{Figures/03-quicksort-illustration3.pdf}}

% Gaetano -> à modifier pour mettre les q,r, etc.

\end{frame}

\begin{frame}\frametitle{\proc{QuickSort} Algorithm}

  \begin{center}
%    \begin{small}

    %% \fcolorbox{white}{Lightgray}{%
    %% \begin{codebox}
    %%   \Procname{$\proc{Partition}(A,\id{begin},\id{end})$}
    %%   \li $q\gets \id{begin}$
    %%   \li $v\gets A[\id{end}]$
    %%   \li \For $i\gets \id{begin}+1$ \To $\id{end}-1$
    %%   \li \Do \If $A[i]\le v$
    %%   \li     \Then $\id{swap}(A[i], A[q])$
    %%   \li           $q\gets q+1$
    %%           \End
    %%       \End
    %%   \li $\id{swap}(A[\id{end}], A[q])$
    %%   \li \Return $q$
    %% \end{codebox}}

    %% \bigskip
    \fcolorbox{white}{Lightgray}{
    \begin{codebox}
      \Procname{$\proc{QuickSort}(A,\id{p},\id{r})$}
      \li \If $\id{p}<\id{r}$
      \li \Then $q\gets\proc{Partition}(A,\id{p},\id{r})$
      \li $\proc{QuickSort}(A,\id{p},q-1)$
      \li $\proc{QuickSort}(A,q+1,\id{r})$
      \End
    \end{codebox}}
%    \end{small}

\bigskip

Appel initial: $\proc{Quicksort}(A,1,\attrib{A}{length})$
  \end{center}
\end{frame}

\begin{frame}{Partition: principe}

\centerline{\includegraphics[width=6cm]{Figures/03-partition-illustration.pdf}}

\bigskip

\begin{itemize}
\item On sélectionne le dernier élément $A[r]$ comme le {\bf pivot}
\item On initialise un indice $i$ à $p-1$
\item On parcourt le tableau de gauche à droite avec un indice $j=p \To r-1$
\item Si $A[j]\le A[r]$, on incrémente $i$ et on échange $A[j]$ et $A[i]$
\item En sortie de boucle, on échange $A[i+1]$ et $A[r]$ et on renvoie $i+1$
\end{itemize}
\end{frame}

\begin{frame}{Partition: illustration}

\begin{columns}
\begin{column}{4cm}
\centerline{\includegraphics[width=3.5cm]{Figures/03-partition2.pdf}}
\end{column}
\begin{column}{6cm}\small
$A[r]$ est le pivot\\
$A[p\twodots i]$ contient des éléments $\leq$ au pivot\\
$A[i+1\twodots j-1]$ contient des éléments $>$ que le pivot\\
$A[j\twodots r-1]$ est la partie du tableau non encore examinée
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Partition: pseudo-code}

\begin{center}
    \fcolorbox{white}{Lightgray}{%
    \begin{codebox}
      \Procname{$\proc{Partition}(A,\id{p},\id{r})$}
      \li $x\gets A[\id{r}]$
      \li $i\gets p-1$
      \li \For $j\gets \id{p}$ \To $\id{r}-1$
      \li \Do \If $A[j]\le x$
      \li     \Then $i\gets i+1$
      \li           $\id{swap}(A[i], A[j])$
              \End
          \End
      \li $\id{swap}(A[\id{i}+1], A[r])$
      \li \Return $i+1$
    \end{codebox}}
\end{center}

\end{frame}

\begin{frame}{Partition: correction}

\begin{center}\small
    \fcolorbox{white}{Lightgray}{%
    \begin{codebox}
      \Procname{$\proc{Partition}(A,\id{p},\id{r})$}
      \li $x\gets A[\id{r}]$
      \li $i\gets p-1$
      \li \For $j\gets \id{p}$ \To $\id{r}-1$
      \li \Do \If $A[j]\le x$
      \li     \Then $i\gets i+1$
      \li           $\id{swap}(A[i], A[j])$
              \End
          \End
      \li $\id{swap}(A[\id{i}+1], A[r])$
      \li \Return $i+1$
    \end{codebox}}
\end{center}

{\bf Pré-condition:} $\{A[p\twodots r],\mbox{ un tableau de nombres}\}$\\
{\bf Post-condition:} $\{A[p\twodots i]\le A[i+1]< A[i+2\twodots r]\}$\\
{\bf Invariant:}
\begin{enumerate}
\item Les valeurs dans $A[p\twodots i]$ sont $\leq$ au pivot
\item Les valeurs dans $A[i+1\twodots j-1]$ sont $>$ que le pivot
\item $A[r]=pivot$
\end{enumerate}

\end{frame}


\begin{frame}{Partition: correction}

{\bf Avant la boucle:} $i=p-1$ et $j=p$ $\Rightarrow$ $A[p\twodots i]$ et $A[i+1\twodots j-1]$ sont vides

\medskip

{\bf Pendant la boucle:}
\centerline{\includegraphics[width=5cm]{Figures/03-partition-illustration.pdf}}

\begin{itemize}
\item Si $A[j]>x$, on incrémente juste $j$. Donc si l'invariant était vrai avant l'exécution du corps, il reste vrai après.
\item Si $A[j]\le x$, on échange $A[j]$ et $A[i+1]$ et $i$ et $j$ sont incrémentés. L'invariant reste donc vérifié également
\end{itemize}

\medskip

{\bf Après la boucle:} 
\centerline{\includegraphics[width=5cm]{Figures/03-fininvariant.pdf}}

En sortie de boucle, l'invariant est vérifié et
on a $j=r$. Echanger $A[i+1]$ et $A[r]$ établit la post-condition.

\end{frame}

\begin{frame}{Algorithme complet}
  \begin{center}
   \begin{small}

\fcolorbox{white}{Lightgray}{%
    \begin{codebox}
      \Procname{$\proc{Partition}(A,\id{p},\id{r})$}
      \li $x\gets A[\id{r}]$
      \li $i\gets p-1$
      \li \For $j\gets \id{p}$ \To $\id{r}-1$
      \li \Do \If $A[j]\le x$
      \li     \Then $i\gets i+1$
      \li           $\id{swap}(A[i], A[j])$
              \End
          \End
      \li $\id{swap}(A[\id{i}+1], A[r])$
      \li \Return $i+1$
    \end{codebox}}

\bigskip

    \fcolorbox{white}{Lightgray}{
    \begin{codebox}
      \Procname{$\proc{QuickSort}(A,\id{p},\id{r})$}
      \li \If $\id{p}<\id{r}$
      \li \Then $q\gets\proc{Partition}(A,\id{p},\id{r})$
      \li $\proc{QuickSort}(A,\id{p},q-1)$
      \li $\proc{QuickSort}(A,q+1,\id{r})$
      \End
    \end{codebox}}
    \end{small}
  \end{center}
\note{\centerline{\includegraphics[width=5cm]{Figures/03-partition-altern.pdf}}}
\end{frame}

\begin{frame}{Illustration}

\centerline{\includegraphics[width=8cm]{Figures/03-quicksort-illustration2-2.pdf}}

\end{frame}

\begin{frame}{Complexité de $\proc{Partition}$}

\begin{center}
    \fcolorbox{white}{Lightgray}{%
    \begin{codebox}
      \Procname{$\proc{Partition}(A,\id{p},\id{r})$}
      \li $x\gets A[\id{r}]$
      \li $i\gets p-1$
      \li \For $j\gets \id{p}$ \To $\id{r}-1$
      \li \Do \If $A[j]\le x$
      \li     \Then $i\gets i+1$
      \li           $\id{swap}(A[i], A[j])$
              \End
          \End
      \li $\id{swap}(A[\id{i}+1], A[r])$
      \li \Return $i+1$
    \end{codebox}}

\end{center}

$$T(n)=\Theta(n)$$

\end{frame}

\begin{frame}{Complexité de $\proc{QuickSort}$}

\begin{center}
    \fcolorbox{white}{Lightgray}{
    \begin{codebox}
      \Procname{$\proc{QuickSort}(A,\id{p},\id{r})$}
      \li \If $\id{p}<\id{r}$
      \li \Then $q\gets\proc{Partition}(A,\id{p},\id{r})$
      \li $\proc{QuickSort}(A,\id{p},q-1)$
      \li $\proc{QuickSort}(A,q+1,\id{r})$
      \End
    \end{codebox}}
\end{center}

\begin{itemize}
\item Pire cas:\hfill{\it (quand se produit-il ?)}

\begin{itemize}
\item $q=p$ ou $q=r$ 
\item Le partitionnement transforme un problème de taille $n$ en un problème de taille $n-1$
$$T(n)=T(n-1)+\Theta(n)$$
\item Même complexité que le tri par insertion:
$$T(n)=\Theta(n^2)$$
\end{itemize}
\end{itemize}
\note{complexité$=\sum_{i=1}^n i$

\bigskip

Ca se produit quand ? (tableau déjà trié). Tri par insertion: tableau trié en ordre inverse.
}
\end{frame}

\begin{frame}{Complexité de \proc{QuickSort}}

\begin{center}
    \fcolorbox{white}{Lightgray}{
    \begin{codebox}
      \Procname{$\proc{QuickSort}(A,\id{begin},\id{end})$}
      \li \If $\id{begin}<\id{end}$
      \li \Then $q\gets\proc{Partition}(A,\id{begin},\id{end})$
      \li $\proc{QuickSort}(A,\id{begin},q-1)$
      \li $\proc{QuickSort}(A,q+1,\id{end})$
      \End
    \end{codebox}}
\end{center}

\begin{itemize}
\item Meilleur cas:
\begin{itemize}
\item $q=\lfloor n/2 \rfloor$
\item Le partitionnement transforme un problème de taille $n$ en deux
  problèmes de taille $\lceil n/2\rceil$ et $\lfloor n/2 \rfloor-1$ respectivement
$$T(n)=2 T(n/2)+\Theta(n)$$

\item Même complexité que le tri par fusion:
$$T(n)=\Theta(n\log n)$$
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Complexité moyenne de \proc{QuickSort}}
\begin{itemize}
\item Complexité moyenne identique à la complexité du meilleur cas
$$T(n)=\Theta(n\log n)$$
\item Intuitivement:
\begin{itemize}
\item En moyenne, on s'attend à une alternance de ``bons'' et de
  ``mauvais'' partitionnements
\item La complexité d'un mauvais partitionnement suivi d'un bon est
  identique à la complexité d'une bon partitionnement directement
  (seule la constante est modifiée).
\end{itemize}
\end{itemize}
\centerline{\includegraphics[width=10cm]{Figures/03-quicksort-casmoyen.pdf}}

\note{Peut-être dire aussi qu'un partitionnement très déséquilibré (1/10 - 9/10) garde une complexité de $n\log n$}

\end{frame}

\begin{frame}{Variantes de \proc{Quicksort}}

\begin{itemize}
\item Choix du pivot:
\begin{itemize}
\item Prendre un élément au hasard plutôt que le dernier.
\item Prendre la médiane de 3 éléments
\item Diminue drastiquement les chances d'être dans le pire cas
\end{itemize}
\end{itemize}

\begin{small}
\begin{center}
    \fcolorbox{white}{Lightgray}{%
     \begin{codebox}
       \Procname{$\proc{Randomized-Partition}(A,\id{p},\id{r})$}
       \li $i=\proc{Random}(p,r)$
       \li $\id{swap}(A[end],A[i])$
       \li \Return $\proc{Partition}(A,p,r)$
     \end{codebox}}

\bigskip

    \fcolorbox{white}{Lightgray}{%
     \begin{codebox}
       \Procname{$\proc{Median-Of-3-Partition}(A,\id{p},\id{r})$}
       \li $i=\proc{median}(A,p,\lfloor (p+r)/2\rfloor,r)$
       \li $\id{swap}(A[r],A[i])$
       \li \Return $\proc{Partition}(A,p,r)$
     \end{codebox}}
\end{center}
\end{small}

\end{frame}

\begin{frame}{Variantes de \proc{Quicksort}}

\begin{itemize}
\item Petits sous-tableaux
\begin{itemize}
\item \proc{Quicksort} est trop lourd pour des petits tableaux
\item Utiliser un tri naïf (par ex., par insertion) sur les sous-tableaux de longueur inférieure à $k$ ($k\approx 20$).
\end{itemize}
\end{itemize}

\begin{small}
\begin{center}
    \fcolorbox{white}{Lightgray}{
    \begin{codebox}
      \Procname{$\proc{QuickSort}(A,\id{p},\id{r})$}
      \li \If $r-p+1 \leq \const{CUTOFF}$
      \li \Then $\proc{InsertionSort}(A,\id{p},\id{r})$
      \li \Return \End
      %\li \If $\id{begin}<\id{end}$
      \li $q\gets\proc{Partition}(A,p,r)$
      \li $\proc{QuickSort}(A,p,q-1)$
      \li $\proc{QuickSort}(A,q+1,r)$
      \End
    \end{codebox}}

\end{center}
\end{small}

\end{frame}

\begin{frame}{Conclusion sur $\proc{QuickSort}$}

\begin{itemize}
\item Rapide en moyenne $\Theta(n\log n)$
\item Pire cas en $\Theta(n^2)$ mais très improbable avec choix du pivot bien fait
\item Bonne performance au niveau du cache
\item Tri \alert{en place} (mais utilise de la mémoire pour la trace récursive)
\item \alert{Pas stable}
\item En pratique souvent un peu plus rapide que \proc{Merge-Sort}
\item Complexité en espace $O(\log n)$ si bien implémenté (récursif terminal, en développant d'abord la partition la plus petite)
\end{itemize}

\end{frame}

\begin{frame}{Jusqu'ici}

  \begin{center}
    \def\arraystretch{1.5}
  \begin{tabular}{@{}lccc@{}c@{}}
    \emph{Algorithme}&\multicolumn{3}{c}{\emph{Complexité}}&\emph{En place?}\\
    & \emph{\small Pire} & \emph{\small Moyenne} & \emph{Meilleure} & \\
    \hline\hline
    \proc{Insertion-Sort}&$\Theta(n^2)$&$\Theta(n^2)$&$\Theta(n)$&oui\\
    \hline
    \proc{Selection-Sort}&$\Theta(n^2)$&$\Theta(n^2)$&$\Theta(n^2)$&oui\\
    \hline
    \proc{Bubble-Sort}&$\Theta(n^2)$&$\Theta(n^2)$&$\Theta(n)$&oui\\
    \hline
    \proc{Merge-Sort}&$\Theta(n\log{n})$&$\Theta(n\log{n})$&$\Theta(n\log{n})$&non\\
    \hline
    \proc{QuickSort} & $\Theta(n^2)$ & $\Theta(n\log{n})$ & $\Theta(n\log{n})$ & oui\\
    \hline\hline
    \hspace{1em}\alert{??}&\alert{$\Theta(n\log{n})$}& & &\alert{oui}\\
    \hline\hline
  \end{tabular}
  \end{center}

\end{frame}

\section{Tri par tas}

\begin{frame}{Tri par tas: introduction}

\begin{itemize}
\item \emph{Heapsort} en anglais
\item inventé par Williams en 1964
\item basé sur une structure de données très utile, le \emph{tas}
\item complexité bornée par $\Theta(n\log n)$ (dans tous les cas)
\item tri en place
\item mise en oeuvre très simple

\bigskip

\item Suite du cours:
\begin{itemize}
\item Introduction aux arbres
\item Tas
\item Tri par tas
\end{itemize}
\end{itemize}

% Voir slides français.

\note{On va d'abord voir le tas, puis le heapsort, On verra la semaine
  prochaine comment utiliser le tas pour faire une file à priorités}
\end{frame}

\subsection{Introduction aux arbres}

\begin{frame}{Arbres: définition}
\begin{itemize}
\item Définition: Un arbre (\emph{tree}) $T$ est un graphe dirigé $(N,E)$, où:
\begin{itemize}
\item $N$ est un ensemble de n\oe uds, et
\item $E\subset N\times N$ est un ensemble d'arcs,
\end{itemize}
possédant les propriétés suivantes:
\begin{itemize}
\item T est connexe et acyclique
\item Si $T$ n'est pas vide, alors il possède un n\oe ud distingué appelé racine (\emph{root node}). Cette racine est unique.
\item Pour tout arc $(n_1,n_2)\in E$, le n\oe ud $n_1$ est le \alert{parent} de $n_2$.
\begin{itemize}
\item La racine de $T$ ne possède pas de parent.
\item Les autres n\oe uds de $T$ possèdent un et un seul parent.
\end{itemize}
\end{itemize}
\end{itemize}

\centerline{\includegraphics[width=5cm]{Figures/03-exemple-arbre2.pdf}}

\end{frame}

\begin{frame}{Arbres: terminologie}
\begin{itemize}
\item Si $n_2$ est le parent de $n_1$, alors $n_1$ est le \alert{fils} (\emph{child}) de $n_2$.
\item Deux n\oe uds $n_1$ et $n_2$ qui possèdent le même parent sont
  des \alert{frères} (\emph{siblings}).
\item Un n\oe ud qui possède au moins un fils est un n\oe ud \alert{interne}.
\item Un n\oe ud externe (c'est-à-dire, non interne) est une \alert{feuille}
  (\emph{leaf}) de l'arbre.
\item Un n\oe ud $n_2$ est un \alert{ancêtre} (\emph{ancestor}) d'un n\oe ud
  $n_1$ si $n_2$ est le parent de $n_1$ ou un ancêtre du parent de $n_1$.
\item Un n\oe ud $n_2$ est un \alert{descendant} d'un n\oe ud $n_1$ si $n_1$ est un ancêtre de $n_2$.
\end{itemize}

\centerline{\includegraphics[width=5cm]{Figures/03-exemple-arbre2.pdf}}

\end{frame}

\begin{frame}{Arbres: terminologie}

\begin{itemize}
\item Un \alert{chemin} est une séquence de n\oe uds $n_1$, $n_2$, \ldots, $n_m$ telle que pour tout $i\in [1,m-1]$, $(n_i,n_{i+1})$ est un arc de l'arbre.\\
Remarque: Il n'existe jamais de chemin reliant deux feuilles distinctes.
\item La \alert{hauteur} (\emph{height}) d'un n\oe ud $n$ est le nombre d'arcs d'un plus long chemin de ce n\oe ud vers une feuille. La \emph{hauteur de l'arbre} est la hauteur de sa racine.
\item La \alert{profondeur} (\emph{depth}) d'un n\oe ud $n$ est le nombre d'arcs sur le chemin qui le relie à la racine.
\end{itemize}

\centerline{\includegraphics[width=5cm]{Figures/03-exemple-arbre2.pdf}}

\end{frame}

\begin{frame}{Arbre binaire}

\begin{itemize}
\item Un arbre \alert{ordonné} est un arbre dans lequel les ensembles de fils de chacun de ses n\oe uds sont ordonnés.
\item Un arbre \alert{binaire} est un arbre ordonné possédant les propriétés suivantes:
\begin{itemize}
\item Chacun de ses n\oe uds possède au plus deux fils.
\item Chaque n\oe ud fils est soit un fils gauche, soit un fils droit.
\item Le fils gauche précède le fils droit dans l'ordre des fils d'un n\oe ud.
\end{itemize}
\item Un arbre \alert{binaire entier} ou {propre} (\emph{full} or \emph{proper}) est un arbre binaire dans lequel tous les n\oe uds internes possèdent exactement deux fils.
\item Un arbre \alert{binaire parfait} est un arbre binaire entier dans lequel toutes les feuilles sont à la même profondeur.
\end{itemize}

\end{frame}

\begin{frame}{Propriétés des arbres binaires entiers}
\label{sec3:proparbres}
% fig: Dupont
\centerline{\includegraphics[width=4.5cm]{Figures/03-arbrebinaireentier.pdf}}

% text: Dupont
\begin{itemize}
\item Le nombre de n\oe uds externes est égal au nombre de n\oe uds internes plus 1.
\item Le nombre de n\oe uds internes est égal à $\frac{n-1}{2}$, où
  $n$ désigne le nombre de n\oe uds.
\item Le nombre de n\oe uds à la profondeur (ou niveau) $i$ est $\leq 2^i$.
\item La hauteur $h$ de l'arbre est $\leq$ au nombre de n\oe uds internes.
\item Le lien entre hauteur et nombre de n\oe uds peut être résumé comme suit:
\begin{center}
$n\in \Omega(h)\mbox{ et }n\in O(2^h)$ (ou $h\in O(n)\mbox{ et }h\in \Omega(\log n)$)
\end{center}
\end{itemize}

\note{
\begin{itemize}
\item Par récursion: vrai pour un seul arbre. Vrai pour gauche et droite, vrai pour l'arbre
\item $n=N_I+N_E=N_I+N_I+1=2 N_I$
\item On multiplie le nombre de noeuds au plus par deux par niveau
\item pire cas est une chaine
\item Prop 4: Chaine $\Rightarrow h\leq N_I=n-1/2\Rightarrow n\geq 2h+1=\Omega(h)$\\
Prop 3: Arbre parfait: $N_E=N_I+1=(n-1)/2\leq 2^h \Rightarrow n=O(h)$
\end{itemize}
}
\end{frame}

\subsection{Tas}

\begin{frame}{Tas: définition}\label{sec:03tas}

Un arbre \alert{binaire complet} est un arbre binaire tel que:
\begin{itemize}
\item Si $h$ dénote la hauteur de l'arbre:
\begin{itemize}
\item Pour tout $i\in [0,h-1]$, il y a exactement $2^i$ n\oe uds à la profondeur $i$.
\item Une feuille a une profondeur $h$ ou $h-1$.
\item Les feuilles de profondeur maximale ($h$) sont ``tassées'' sur la gauche.
\end{itemize}
\end{itemize}
\medskip

Un \alert{tas binaire} (binary heap) est un arbre binaire complet tel que:
\begin{itemize}
\item Chacun de ses n\oe uds est associé à une clé.
\item La clé de chaque n\oe ud est supérieure ou égale à celle de ses
  fils (\alert{propriété d'ordre du tas}).
\end{itemize}

% fig: CLRS
\centerline{\includegraphics[width=5cm]{Figures/03-tas-binaire.pdf}}

\end{frame}

\begin{frame}{Propriété d'un tas}
\label{03:hauteurtas}
\begin{itemize}
\item Soit $T$ un arbre binaire complet contenant $n$ entrées et de hauteur $h$:
\begin{itemize}
\item $n$ est supérieur ou égal à la taille de l'arbre parfait de hauteur $h-1$ plus un, soit $2^{h-1+1}-1+1=2^h$
\item $n$ est inférieur ou égal à la taille de l'arbre parfait de hauteur $h$, soit $2^{h+1}-1$
\begin{eqnarray*}
2^h\leq n\leq 2^{h+1}-1 & \Leftrightarrow & 2^h\leq n < 2^{h+1}\\
& \Leftrightarrow & h \leq \log_2 n < h+1 \\
& \Leftrightarrow & h=\lfloor \log_2 n\rfloor
\end{eqnarray*}
\end{itemize}
\end{itemize}

% fig: CLRS
\centerline{\includegraphics[width=5cm]{Figures/03-tas-binaire.pdf}}

\note{
\begin{itemize}
\item Point 1: hauteur de l'arbre de la figure=3: arbre complet de hauteur
  $h-1\Rightarrow$ $N_E=2^{h-1}$ feuilles. $n=N_E+N_E-1=2*2^{h-1}-1=2^h-1$.\\
$+1$ parce que sinon l'arbre binaire complet serait de profondeur $h-1$.
\item Point 2: idem.
\end{itemize}


}
\end{frame}

%% \begin{frame}{Tas: interface}

%% Opérations définies sur un tas $H$:
%% \begin{itemize}
%% \item $\attrib{H}{heap-size}$: le nombre de clés dans $H$.
%% \item $\proc{Build-Max-Heap}(A)$ construit un tas à partir du tableau $A$.
%% \item $\proc{Heap-Insert}(H,key)$ insère $key$ dans le tas.
%% \item $\proc{Heap-Extract-Max}(H)$ extrait la clé maximale.
%\end{itemize}

%\bigskip

%\centerline{\includegraphics[width=5cm]{Figures/03-tas-binaire.pdf}}

%\end{frame}

\begin{frame}{Implémentation par un tableau}

\centerline{\includegraphics[width=10cm]{Figures/03-tas-binaire-tableau.pdf}}

\bigskip

Un tas peut être représenté de manière compacte à l'aide d'un tableau $A$.

\begin{itemize}
\item La racine de l'arbre est le premier élément du tableau.
\item $\proc{Parent}(i)=\lfloor i/2\rfloor$
\item $\proc{Left}(i)=2i$
\item $\proc{Right}(i)=2i+1$
\end{itemize}

Propriété d'ordre du tas: $\forall i, A[\proc{Parent}(i)]\geq A[i]$

\end{frame}

\begin{frame}{Principe du tri par tas}
\begin{itemize}
\item On construit un tas à partir du tableau à trier $\rightarrow$ $\proc{Build-max-heap}(A)$.
\item Tant que le tas contient des éléments:
\begin{itemize}
\item On extrait l'élément au sommet du tas qu'on place dans le
  tableau trié et on le remplace par l'élément le plus à droite

\centerline{\includegraphics[width=5cm]{Figures/03-heap-extract-max2.pdf}~~~~\includegraphics[width=5cm]{Figures/03-heap-extract-max2-2.pdf}}

\item On rétablit la propriété de tas en tenant compte du fait que les sous-arbres de droite et de gauche sont des tas $\rightarrow$ $\proc{Max-Heapify}(A,1)$ 
\end{itemize}
\end{itemize}
(Tout se fait dans le tableau initial $\rightarrow$ en place)
\end{frame}

%% \begin{frame}{$\proc{Heap-Extract-Max}$}

%% \begin{itemize}
%% \item Procédure $\proc{Heap-Extract-Max}$:
%% \begin{itemize}
%% \item Extrait la clé maximale. Elle est toujours à la racine \emph{(Pourquoi ?)}
%% \item Réarrange le tas pour maintenir la propriété d'ordre du tas
%% \end{itemize}

%% % FIG: Gaetano -> a refaire
%% \centerline{\includegraphics[width=5cm]{Figures/03-heap-extract-max2.pdf}~~~~\includegraphics[width=5cm]{Figures/03-heap-extract-max2-2.pdf}}

%% \item Principe:
%% \begin{itemize}
%% \item On remplace la racine par la feuille la plus à droite ($A[1]=A[N]$)
%% \item Réarrange le tas en tenant compte du fait que les sous-arbres de droite et de gauche sont des tas (appel de $\proc{Max-Heapify}(A,1)$)
%% \end{itemize}
%% \end{itemize}
%% \end{frame}

\begin{frame}{$\proc{Max-Heapify}$}

\begin{itemize}
\item Procédure $\proc{Max-Heapify}(A,i)$:
\begin{itemize}
\item Suppose que le sous-arbre de gauche du n\oe ud $i$ est un tas
\item Suppose que le sous-arbre de droite du n\oe ud $i$ est un tas
\item But: réarranger le tas pour maintenir la propriété d'ordre du tas
\end{itemize}
\item Ex: $\proc{Max-Heapify}(A,2)$
\centerline{\includegraphics[width=8cm]{Figures/03-max-heapify.pdf}}
\end{itemize}

\note{
\begin{itemize}
\item On prend le plus grand des voisins de gauche et de droite
\item On le swap avec la racine.
\item On appelle la procédure récursivement sur le fils modifié
\end{itemize}
}
\end{frame}

\begin{frame}{$\proc{Max-Heapify}$}

\begin{center}
\begin{small}
\fcolorbox{white}{Lightgray}{
      \begin{codebox}
        \Procname{$\proc{Max-Heapify}(A,i)$}
        \li $l\gets\proc{Left}(i)$
        \li $r\gets\proc{Right}(i)$
        \li \If $l\le \attrib{A}{heap-size} \wedge A[l]>A[i]$
        \li \Then $\id{largest}\gets l$
        \li \Else $\id{largest}\gets i$
        \End
        \li \If $r\le \attrib{A}{heap-size} \wedge A[r]>A[\id{largest}]$
        \li \Then $\id{largest}\gets r$
        \End
        \li \If $\id{largest}\ne i$
        \li \Then $\id{swap}(A[i],A[\id{largest}])$
        \li       $\proc{Max-Heapify}(A,\id{largest})$
        \End
      \end{codebox}}
\end{small}
\end{center}

\bigskip

\begin{itemize}
\item Complexité ? La hauteur du n\oe ud: $T(n)=O(\log n)$ (Transp. \pageref{03:hauteurtas})
\end{itemize}

\note{On vérifie qu'il y a bien un fils gauche et un fils de droite}

\end{frame}

\begin{frame}{Construction d'un tas}

\begin{center}\small
\fcolorbox{white}{Lightgray}{
      \begin{codebox}
        \Procname{$\proc{Build-Max-Heap}(A)$}
        \li $\attrib{A}{heap-size}\gets\attrib{A}{length}$
        \li \For $i\gets\lfloor \attrib{A}{length}/2\rfloor$ \Downto $1$
        \li \Do $\proc{Max-Heapify}(A,i)$
        \End
      \end{codebox}}
\end{center}
{\footnotesize (Invariant: chaque n\oe ud i, i+1,\ldots, n est la racine d'un tas)}
\bigskip

\centerline{\includegraphics[width=10cm]{Figures/03-build-max-heap.pdf}}

\bigskip

\begin{itemize}
\item La tableau initial est interprété comme un arbre binaire complet
\item On tasse les n\oe uds internes de bas en haut et de droite à gauche
\end{itemize}

\note{Montrer au tableau comment ça marche

\bigskip

borne à $length(A)/2$ car fils gauche d'un n\oe ud $i$ est donné par $2*i$. Pas de n\oe ud si $2i>A.length \Leftrightarrow i>A.length/2$.


}

\end{frame}

\begin{frame}{Complexité de $\proc{Build-Max-Heap}$}

\begin{itemize}
\item Borne simple:
\begin{itemize}
\item $O(n)$ appels à $\proc{Max-Heapify}$, chacun étant $O(\log n)$ $\Rightarrow O(n\log n)$.
\end{itemize}

\bigskip


\item Analyse plus fine:
\begin{itemize}
\item Pour simplifier l'analyse, on suppose que l'arbre binaire est parfait.
\item On a donc $n=2^{h+1}-1$ pour un $h\geq 0$, qui est aussi la hauteur de l'arbre résultant
\end{itemize}
\end{itemize}

\note{ $2^{h+1}-1$ $\Rightarrow$ car le nombre de feuille est $2^h$ et
  donc le nombre total de n\oe ud est $2 2^{h}-1=2^{h+1}-1$ }

\end{frame}

\begin{frame}{Complexité de $\proc{Build-Max-Heap}$}
\centerline{\includegraphics[width=10cm]{Figures/03-max-heapify-complex2.pdf}}

\begin{itemize}
\item Il y a $2^i$ n\oe uds à la profondeur $i$ (= hauteur $h-i$).
\item On doit appeler $\proc{Max-Heapify}$ sur chacun d'eux $\Rightarrow O(h-i)$.
\item Nombre d'opérations en fonction de $h$:
$$T(h)=\sum_{i=0}^{h-1} 2^i O(h-i)=O(\sum_{i=0}^{h-1} 2^i (h-i))$$
\end{itemize}
\end{frame}

\begin{frame}{Complexité de $\proc{Build-Max-Heap}$}
\begin{itemize}
\item $\sum_{i=0}^{h-1} 2^i (h-i)=1\cdot 2^{h-1}+2\cdot 2^{h-2}+\ldots+(h-1)\cdot 2^1+h\cdot 2^0$

\bigskip

\begin{small}
\begin{tabular}{ccccccccccl}
$2^{h-1}$ & + & $2^{h-2}$ & + & $2^{h-3}$ & + & \ldots & +& $2^0$ & = & $2^{h}-1$\\
 & + & $2^{h-2}$ & + & $2^{h-3}$ & + & \ldots & +& $2^0$ & = & $2^{h-1}-1$\\
 & & & + & $2^{h-3}$ & + & \ldots & +& $2^0$ & = & $2^{h-2}-1$\\
 &   &        &   &      &   & \ldots &  &    & \ldots & \\
 &   &        &   &     &    &        & & $2^0$ & = & $2^1-1$\\
\hline
 &   &        &   &     &    &        & &     & = & $\left(\sum_{i=1}^h 2^i\right)-h$
\end{tabular}
\end{small}

\medskip

(En utilisant $\sum_{i=0}^n x^i=\frac{x^{n+1}-1}{x-1}$)

\bigskip

\item On obtient donc $T(h)=O(2^{h+1}-h-2).$
\item Puisque $h=log_2(n+1)-1=O(\log n)$, on a $$T(n)=O(n).$$
\end{itemize}
\end{frame}

%\subsection{File à priorités}

\subsection{Tri par tas}

\begin{frame}{Tri par tas: algorithme}

\begin{center}
\fcolorbox{white}{Lightgray}{
        \begin{codebox}
          \Procname{$\proc{Heap-Sort}(A)$}
          \li $\proc{Build-Max-Heap}(A)$
          \li \For $i\gets\attrib{A}{length}$ \Downto $2$
          \li \Do $\id{swap}(A[i],A[1])$
          \li     $\attrib{A}{heap-size} \gets\attrib{A}{heap-size}-1$
          \li     $\proc{Max-Heapify}(A,1)$
          \End
        \end{codebox}}
\end{center}

\bigskip

{\small
Invariant:\\
$A[1\twodots i]$ est un tas contenant les $i$ éléments les plus petits de $A[1\twodots \attrib{A}{length}]$ et $A[i+1\twodots \attrib{A}{length}]$ contient les $n-i$ éléments les plus grands de $A[1\twodots \attrib{A}{length}]$ triés.
}
\end{frame}

\begin{frame}{Tri par tas: illustration}
Tableau initial: $A=[7,4,3,1,2]$

\bigskip

%Fig:CLRS
\centerline{\includegraphics[width=7cm]{Figures/03-heapsort-exemple.pdf}}

\end{frame}

\begin{frame}{Complexité de \proc{Heap-Sort}}
\begin{center}\small
\fcolorbox{white}{Lightgray}{
        \begin{codebox}
          \Procname{$\proc{Heap-Sort}(A)$}
          \li $\proc{Build-Max-Heap}(A)$
          \li \For $i\gets\attrib{A}{length}$ \Downto $2$
          \li \Do $\id{swap}(A[i],A[1])$
          \li     $\attrib{A}{heap-size} \gets\attrib{A}{heap-size}-1$
          \li     $\proc{Max-Heapify}(A,1)$
          \End
        \end{codebox}}
\end{center}

\begin{itemize}
\item $\proc{Build-Max-Heap}$: $O(n)$
\item Boucle $\For$: $n-1$ fois
\item Echange d'éléments: $O(1)$
\item $\proc{Max-Heapify}$: $O(\log n)$
\end{itemize}
Total: $O(n\log n)$ \alert{(pour le pire cas et le cas moyen)}

\bigskip

Le tri par tas est cependant généralement battu par le tri rapide
\end{frame}

\section{Synthèse}

\begin{frame}{Résumé}

 \begin{center}
    \def\arraystretch{1.5}
  \begin{tabular}{@{}lccc@{}c@{}}
    \emph{Algorithme}&\multicolumn{3}{c}{\emph{Complexité}}&\emph{En place?}\\
    & \emph{\small Pire} & \emph{\small Moyenne} & \emph{Meilleure} & \\
    \hline\hline
    \proc{Insertion-Sort}&$\Theta(n^2)$&$\Theta(n^2)$&$\Theta(n)$&oui\\
    \hline
    \proc{Selection-Sort}&$\Theta(n^2)$&$\Theta(n^2)$&$\Theta(n^2)$&oui\\
    \hline
    \proc{Bubble-Sort}&$\Theta(n^2)$&$\Theta(n^2)$&$\Theta(n)$&oui\\
    \hline
    \proc{Merge-Sort}&$\Theta(n\log{n})$&$\Theta(n\log{n})$&$\Theta(n\log{n})$&non\\
    \hline
    \proc{Quick-Sort} & $\Theta(n^2)$ & $\Theta(n\log{n})$ & $\Theta(n\log{n})$ & oui\\
    \hline
    \proc{Heap-Sort} & $\Theta(n\log{n})$ & $\Theta(n\log{n})$ & $\Theta(n\log{n})^*$ & oui\\
    \hline\hline
  \end{tabular}\\
\medskip
{~\hfill\footnotesize $^*$ pas montré dans ce cours}
  \end{center}

\end{frame}

\begin{frame}{Peut-on faire mieux que $O(n\log n)$?}

% Gaetano

\begin{itemize}
\item Non, si on se restreint aux tri \emph{comparatifs}, c'est-à-dire:
\begin{itemize}
\item Aucune hypothèse sur les éléments à trier
\item Nécessité de les comparer entre eux
\end{itemize}
\item Complexité d'un problème algorithmique versus complexité d'un algorithme
\item Dans ce cas, un algorithme de tri est:
\begin{itemize}
\item Une suite de comparaisons d'éléments suivant une certaine méthode
\item Un processus qui transforme un tableau $[e_0,e_1,\ldots,e_{n-1}]$ en un autre tableau $[e_{\sigma_0},e_{\sigma_1},\ldots,e_{\sigma_{n-1}}]$ où $(\sigma_0,\sigma_1,\ldots,\sigma_{n-1})$ est une permutation de $(0,1,\ldots,n-1)$.
\end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Arbre de décision: exemple}

Un algorithme de tri = un arbre binaire de décision (entier)

\bigskip

\centerline{\includegraphics[width=8cm]{Figures/03-decisioninsertion.pdf}}

\bigskip

(arbre de décision pour le tri par insertion du tableau $[e_0,e_1,e_2]$)

\bigskip

\emph{Exercice: construire l'arbre pour le tri par fusion}

\end{frame}

\begin{frame}{Arbre de décision: définition}

Un algorithme de tri = un arbre binaire de décision

\begin{itemize}
\item feuille de l'arbre: une permutation des éléments du tableau initial
\item tri: le chemin de la racine à la feuille correspondant au tableau trié
\item hauteur de l'arbre: le pire cas pour le tri
\item branche la plus courte: le meilleur cas pour le tri
\item hauteur moyenne de l'arbre: la complexité en moyenne du tri
\end{itemize}

\end{frame}

\begin{frame}{Arbre de décision: propriété}
\begin{itemize}
\item Un arbre binaire de hauteur $h$ a au plus $2^h$ feuilles (cf transp. \pageref{sec3:proparbres})
\item Le nombre de feuilles de l'arbre de décision est  exactement $n!$ où  $n$ est la taille du tableau à trier\\
\emph{(par l'absurde: si moins que $n!$ certains tableaux ne seraient pas correctement triés)}
% 
\item On a donc:
$$n!\leq 2^h \Rightarrow \log(n!)\leq h$$
\item Formule de Stirling:
$$n!=\sqrt{2\pi n} (\frac{n}{e})^n (1+\Theta(\frac{1}{n}))\Rightarrow n!\geq (\frac{n}{e})^n$$

$$h\geq \log(n!)>\log((\frac{n}{e})^n)=n\log n - n\log e\Rightarrow h=\Omega(n\log n)$$

\end{itemize}
\bigskip

\centerline{\bf Le problème du tri comparatif est $\Omega(n\log n)$}

\note{Dire qu'il ne peut pas y en avoir moins que $n!$ feuilles sinon, un cas ne serait pas traiter}

\end{frame}

\begin{frame}{Ce qu'on a vu}

\begin{itemize}
\item Catégorisation des algorithmes de tri
\item \proc{QuickSort} (tri en place en $O(n\log n))$
\item Analyse du cas moyen d'un algorithme
\item Notre première structure de données: le tas
\item \proc{HeapSort} (tri en place en $\Theta(n\log n)$)
\item Borne inférieure sur les tris comparatifs

\bigskip

\item Illustration:
\begin{itemize}
\item \url{http://www.sorting-algorithms.com/}
\end{itemize}

\end{itemize}

\end{frame}

\begin{frame}{Ce qu'on n'a pas vu}

\begin{itemize}
\item Analyse formelle de la complexité moyenne du $\proc{QuickSort}$.
\item Invariant pour le tri par tas
\item Méthodes de tri linéaire
\item Méthodes de sélection: trouver l'élément de rang $i$
\end{itemize}

\note{Demander comment il ferait ça de manière naive

\bigskip

Tri par comptage: suppose qu'on trie des entiers et qu'on connait le min et le max: on fait un histogramme et puis on parcourt cet histogramme}
\end{frame}
